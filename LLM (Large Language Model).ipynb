{"cells":[{"cell_type":"code","execution_count":6,"id":"62043ba0","metadata":{"id":"62043ba0","executionInfo":{"status":"ok","timestamp":1729738437464,"user_tz":-540,"elapsed":353,"user":{"displayName":"전윤범","userId":"16579588953913211126"}}},"outputs":[],"source":["# 간단한 Transformer 기반 LLM 모델을 구축하는 파이썬 코드입니다.\n","# 이 코드는 GPT와 같은 언어 모델의 기본적인 아키텍처를 보여줍니다."]},{"cell_type":"code","execution_count":6,"id":"93aa2cc8","metadata":{"id":"93aa2cc8","executionInfo":{"status":"ok","timestamp":1729738437796,"user_tz":-540,"elapsed":12,"user":{"displayName":"전윤범","userId":"16579588953913211126"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":6,"id":"19170317","metadata":{"id":"19170317","executionInfo":{"status":"ok","timestamp":1729738437797,"user_tz":-540,"elapsed":10,"user":{"displayName":"전윤범","userId":"16579588953913211126"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":6,"id":"ccc033f2","metadata":{"id":"ccc033f2","executionInfo":{"status":"ok","timestamp":1729738437797,"user_tz":-540,"elapsed":9,"user":{"displayName":"전윤범","userId":"16579588953913211126"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":7,"id":"b1d68309","metadata":{"id":"b1d68309","executionInfo":{"status":"ok","timestamp":1729738437797,"user_tz":-540,"elapsed":8,"user":{"displayName":"전윤범","userId":"16579588953913211126"}}},"outputs":[],"source":["# 코드 설명\n","# Multi-Head Attention:\n","# MultiHeadAttention 클래스는 GPT와 같은 모델의 핵심 요소입니다. 여러 개의 어텐션 헤드를 이용해 병렬로 어텐션을 수행한 후, 이를 결합하여 최종 출력을 만듭니다.\n","\n","# Decoder Layer:\n","# GPT는 디코더 기반 모델이므로, DecoderLayer 클래스는 Multi-Head Attention과 Feed Forward Network로 구성된 하나의 디코더 층입니다.\n","# 이 층에서는 Layer Normalization과 Residual 연결이 적용됩니다.\n","\n","# GPT 모델:\n","\n","# GPTModel 클래스는 여러 개의 디코더 층을 쌓아 구성한 전체 모델입니다. 각 층에서 입력 시퀀스가 처리됩니다.\n","# 이 모델은 **임베딩 층(Embedding Layer)**와 Positional Encoding을 사용하여 입력 시퀀스를 벡터로 변환하고, 이를 디코더에 전달합니다.\n","\n","# Positional Encoding:\n","\n","# Transformer는 시퀀스의 순서를 모델에 전달하기 위해 Positional Encoding을 추가합니다. 이는 각 위치 정보를 사인(sin)과 코사인(cos) 함수로 표현하여, 위치 정보를 학습할 수 있도록 합니다.\n","\n","# GPT 구조의 특징\n","# Unidirectional: GPT는 한 방향성을 가지며, 이전 시점의 정보만을 사용해 다음 시점을 예측합니다. 이는 Masked Self-Attention을 통해 구현됩니다.\n","# 디코더만 사용: GPT는 전체 구조에서 디코더 부분만 사용하며, 언어 생성 작업에 특화되어 있습니다."]},{"cell_type":"code","execution_count":7,"id":"4697c2d6","metadata":{"id":"4697c2d6","executionInfo":{"status":"ok","timestamp":1729738437797,"user_tz":-540,"elapsed":7,"user":{"displayName":"전윤범","userId":"16579588953913211126"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":7,"id":"1cadaee7","metadata":{"id":"1cadaee7","executionInfo":{"status":"ok","timestamp":1729738437797,"user_tz":-540,"elapsed":6,"user":{"displayName":"전윤범","userId":"16579588953913211126"}}},"outputs":[],"source":[]},{"cell_type":"markdown","id":"302d2fb4","metadata":{"id":"302d2fb4"},"source":["# 종합"]},{"cell_type":"code","execution_count":8,"id":"3af07f75","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2931,"status":"ok","timestamp":1729738440722,"user":{"displayName":"전윤범","userId":"16579588953913211126"},"user_tz":-540},"id":"3af07f75","outputId":"74bbe2a5-9f05-4887-9490-42a9946d843a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Name: tensorflow\n","Version: 2.17.0\n","Summary: TensorFlow is an open source machine learning framework for everyone.\n","Home-page: https://www.tensorflow.org/\n","Author: Google Inc.\n","Author-email: packages@tensorflow.org\n","License: Apache 2.0\n","Location: /usr/local/lib/python3.10/dist-packages\n","Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, requests, setuptools, six, tensorboard, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n","Required-by: dopamine_rl, tf_keras\n","Python 3.10.12\n"]}],"source":["!pip show tensorflow\n","!python --version"]},{"cell_type":"code","execution_count":9,"id":"41771dd3","metadata":{"id":"41771dd3","outputId":"3017ef13-b79f-449c-d50d-d6bc11d42e09","executionInfo":{"status":"ok","timestamp":1729738440724,"user_tz":-540,"elapsed":11,"user":{"displayName":"전윤범","userId":"16579588953913211126"}},"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.10.12\n"]}],"source":["!python --version\n"]},{"cell_type":"code","execution_count":10,"id":"2fa564df","metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1729738440726,"user":{"displayName":"전윤범","userId":"16579588953913211126"},"user_tz":-540},"id":"2fa564df"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Embedding, LayerNormalization, Dropout\n","from tensorflow.keras.models import Model"]},{"cell_type":"code","execution_count":11,"id":"42ef9209","metadata":{"id":"42ef9209","executionInfo":{"status":"ok","timestamp":1729738440727,"user_tz":-540,"elapsed":11,"user":{"displayName":"전윤범","userId":"16579588953913211126"}}},"outputs":[],"source":["# Scaled Dot-Product Attention\n","def scaled_dot_product_attention(q, k, v, mask=None):\n","    matmul_qk = tf.matmul(q, k, transpose_b=True)\n","    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","    if mask is not None:\n","        scaled_attention_logits += (mask * -1e9)\n","\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n","    output = tf.matmul(attention_weights, v)\n","    return output, attention_weights"]},{"cell_type":"code","execution_count":12,"id":"8caae8da","metadata":{"id":"8caae8da","executionInfo":{"status":"ok","timestamp":1729738440727,"user_tz":-540,"elapsed":11,"user":{"displayName":"전윤범","userId":"16579588953913211126"}}},"outputs":[],"source":["\n","\n","\n","# Multi-Head Attention\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","\n","        assert d_model % self.num_heads == 0\n","\n","        self.depth = d_model // self.num_heads\n","\n","        self.wq = Dense(d_model)\n","        self.wk = Dense(d_model)\n","        self.wv = Dense(d_model)\n","        self.dense = Dense(d_model)\n","\n","    def split_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    def call(self, v, k, q, mask):\n","        batch_size = tf.shape(q)[0]\n","\n","        q = self.wq(q)\n","        k = self.wk(k)\n","        v = self.wv(v)\n","\n","        q = self.split_heads(q, batch_size)\n","        k = self.split_heads(k, batch_size)\n","        v = self.split_heads(v, batch_size)\n","\n","        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n","\n","        output = self.dense(concat_attention)\n","        return output, attention_weights\n","\n","# Feed Forward Network\n","def point_wise_feed_forward_network(d_model, dff):\n","    return tf.keras.Sequential([\n","        Dense(dff, activation='relu'),\n","        Dense(d_model)\n","    ])\n","\n","# Transformer Decoder Layer (GPT 구조와 유사)\n","class DecoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(DecoderLayer, self).__init__()\n","\n","        self.mha = MultiHeadAttention(d_model, num_heads)\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = Dropout(rate)\n","        self.dropout2 = Dropout(rate)\n","\n","    def call(self, x, training, mask):\n","        attn_output, _ = self.mha(x, x, x, mask)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(x + attn_output)\n","\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        out2 = self.layernorm2(out1 + ffn_output)\n","\n","        return out2\n","\n","# GPT 모델 (Transformer 기반의 LLM)\n","class GPTModel(Model):\n","    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, max_pos_encoding, rate=0.1):\n","        super(GPTModel, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = Embedding(target_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(max_pos_encoding, d_model)\n","\n","        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","        self.dropout = Dropout(rate)\n","\n","    def call(self, x, training, mask):\n","        seq_len = tf.shape(x)[1]\n","        attention_weights = {}\n","\n","        x = self.embedding(x)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","\n","        x = self.dropout(x, training=training)\n","\n","        for i in range(self.num_layers):\n","            x = self.dec_layers[i](x, training=training, mask=mask)\n","\n","        return x\n","\n","# Positional Encoding\n","import numpy as np\n","\n","def positional_encoding(position, d_model):\n","    angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n","\n","    sines = np.sin(angle_rads[:, 0::2])\n","    cosines = np.cos(angle_rads[:, 1::2])\n","\n","    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n","    pos_encoding = pos_encoding[np.newaxis, ...]\n","\n","    return tf.cast(pos_encoding, dtype=tf.float32)\n","\n","def get_angles(pos, i, d_model):\n","    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n","    return pos * angle_rates"]},{"cell_type":"code","execution_count":13,"id":"7c5cca06","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1021,"status":"ok","timestamp":1729738441737,"user":{"displayName":"전윤범","userId":"16579588953913211126"},"user_tz":-540},"id":"7c5cca06","outputId":"cb95002b-fd9a-4ce4-ddf3-2f57d9d0ca11"},"outputs":[{"output_type":"stream","name":"stdout","text":["(64, 20, 512)\n"]}],"source":["\n","\n","\n","# 샘플 모델 테스트\n","sample_gpt = GPTModel(num_layers=2, d_model=512, num_heads=8, dff=2048, target_vocab_size=8000, max_pos_encoding=5000)\n","\n","temp_input = tf.random.uniform((64, 20), dtype=tf.int64, minval=0, maxval=200)\n","output = sample_gpt(temp_input, training=False, mask=None)\n","print(output.shape)  # (batch_size, input_seq_len, d_model)\n"]},{"cell_type":"code","execution_count":13,"id":"e2fd5192","metadata":{"id":"e2fd5192","executionInfo":{"status":"ok","timestamp":1729738441738,"user_tz":-540,"elapsed":11,"user":{"displayName":"전윤범","userId":"16579588953913211126"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":14,"id":"0ed0c3a5","metadata":{"id":"0ed0c3a5","outputId":"01a5f86c-4164-4d1e-d397-14a2b594e66d","executionInfo":{"status":"ok","timestamp":1729738441739,"user_tz":-540,"elapsed":11,"user":{"displayName":"전윤범","userId":"16579588953913211126"}},"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(64, 20, 512), dtype=float32, numpy=\n","array([[[ 0.52559865, -1.2146963 , -0.04999537, ..., -1.0526516 ,\n","          0.6155404 ,  0.5563539 ],\n","        [-0.20539647, -1.1764107 ,  1.0369886 , ..., -0.75341135,\n","          1.7446352 ,  0.57262176],\n","        [ 0.55239815, -0.303398  , -0.00522766, ..., -1.3012663 ,\n","          2.4997525 ,  0.22025205],\n","        ...,\n","        [-0.6086065 , -1.823865  , -0.32166487, ..., -1.5583954 ,\n","          1.470501  ,  1.3339434 ],\n","        [-0.4292233 , -1.88939   , -1.5638807 , ..., -1.2856418 ,\n","          2.2884107 ,  1.6459411 ],\n","        [-0.32540733, -2.4481504 , -1.4420972 , ..., -0.7427204 ,\n","          1.0035946 ,  1.562907  ]],\n","\n","       [[-0.60038465, -1.2879398 , -0.8882612 , ..., -0.42889184,\n","          1.5921547 ,  0.74801326],\n","        [ 0.9600346 ,  0.7056459 , -0.24339516, ..., -0.23028536,\n","          1.1671729 ,  0.94767964],\n","        [ 0.57203394,  0.60987866,  1.1439922 , ..., -0.30341375,\n","          0.6386689 ,  1.5090677 ],\n","        ...,\n","        [-0.20851718, -1.6931854 , -0.6107919 , ..., -0.74762076,\n","          2.6428468 ,  1.4877431 ],\n","        [-0.2999985 , -1.5161514 , -0.9480335 , ..., -0.64324135,\n","          2.0954595 ,  1.3173648 ],\n","        [-0.04955003, -0.8151367 , -0.50850666, ..., -0.11738259,\n","          1.6848708 ,  1.0017964 ]],\n","\n","       [[ 0.14746265, -1.6242658 , -0.7865714 , ..., -0.46050492,\n","          1.1502882 , -0.4810971 ],\n","        [ 0.39433184, -0.9467777 ,  0.7765844 , ..., -1.3739645 ,\n","          1.8040571 ,  0.05304074],\n","        [ 0.7634022 , -1.3394824 ,  0.18529032, ..., -1.7460382 ,\n","          1.6475835 ,  0.64810485],\n","        ...,\n","        [-0.64668685, -0.77451974,  0.38847026, ..., -0.516514  ,\n","          1.2818763 ,  1.9035656 ],\n","        [ 0.17000163, -0.6998952 , -0.78976303, ..., -0.43304542,\n","          1.6403785 ,  1.8555312 ],\n","        [ 1.0927566 , -1.6370354 , -1.098561  , ..., -1.1631705 ,\n","          1.5157388 ,  1.1465006 ]],\n","\n","       ...,\n","\n","       [[ 0.26324612, -1.3592722 ,  0.20147629, ..., -0.61557543,\n","          1.9852618 ,  0.7944641 ],\n","        [ 0.88279146, -0.82018596,  0.3438295 , ..., -1.0858382 ,\n","          0.6470124 ,  0.50978154],\n","        [ 1.1252747 , -0.5916371 , -0.45267412, ..., -0.97184646,\n","          0.9830743 ,  1.2481891 ],\n","        ...,\n","        [-0.58653307, -1.1336125 ,  0.04519531, ..., -1.0984862 ,\n","          1.2794826 ,  1.2391448 ],\n","        [-0.4061318 , -2.1535296 , -0.76949936, ..., -0.6151594 ,\n","          1.4529438 ,  0.6971578 ],\n","        [-0.05215729, -1.2211392 , -1.4366559 , ..., -2.002885  ,\n","          0.8090321 ,  0.40121594]],\n","\n","       [[-0.03599142, -1.3981028 , -0.17603344, ..., -1.3434494 ,\n","          2.435644  ,  0.17367488],\n","        [ 1.033786  , -1.4199286 ,  0.55680215, ..., -0.77786845,\n","          2.7708743 ,  1.0917839 ],\n","        [ 0.9177984 , -0.6256655 ,  1.0213588 , ..., -1.0477551 ,\n","          2.2528036 ,  0.8125391 ],\n","        ...,\n","        [-0.95035195, -1.0438015 ,  0.3624583 , ..., -0.9880471 ,\n","          1.5551369 ,  2.0271497 ],\n","        [-0.49696815, -2.1084242 , -0.48241466, ..., -0.66510063,\n","          2.1842136 ,  0.76916695],\n","        [ 0.8514028 , -0.5125878 , -0.6419187 , ..., -0.534883  ,\n","          0.9485189 ,  1.6798438 ]],\n","\n","       [[ 0.7538926 , -2.0109966 , -0.34447283, ..., -0.6623968 ,\n","          0.7869895 ,  1.075984  ],\n","        [ 0.7925458 , -1.84235   ,  0.58344966, ..., -0.89904785,\n","          0.68547   ,  0.21384647],\n","        [-0.4929663 , -0.49398917, -1.3556056 , ..., -0.28451574,\n","          0.9873181 ,  1.1479862 ],\n","        ...,\n","        [-0.30486307, -1.6294504 , -0.23716952, ..., -0.83065844,\n","          0.7691244 ,  0.61741966],\n","        [-0.7669851 , -1.4631355 ,  0.02692618, ..., -0.5712621 ,\n","          1.597842  ,  0.79038835],\n","        [ 0.9631991 , -1.0491115 , -1.027142  , ..., -0.24407968,\n","          2.0995026 ,  1.005336  ]]], dtype=float32)>"]},"metadata":{},"execution_count":14}],"source":["output"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":5}