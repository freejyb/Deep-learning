{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc3f238-edad-4063-90cc-b5c518590cfe",
   "metadata": {},
   "source": [
    "# Attention 메커니즘의 핵심 아이디어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90464f17-703a-4a13-9150-f8a5b53796d7",
   "metadata": {},
   "source": [
    "1. 동적 집중(Dynamic Focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cacc7af-d87b-4a6c-acd8-50f769dbd602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights: [[0.4223188 0.1553624 0.4223188]]\n",
      "Dynamic Focus Output: [[6.33478197 3.66521803]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Query, Key, Value 벡터 정의\n",
    "query = np.array([[1, 0, 1]])\n",
    "key = np.array([[1, 0, 0], [0, 1, 0], [1, 1, 0]])\n",
    "value = np.array([[10, 0], [0, 10], [5, 5]])\n",
    "\n",
    "# 어텐션 점수 계산\n",
    "scores = np.dot(query, key.T)\n",
    "\n",
    "# Softmax를 통한 정규화\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "attention_weights = softmax(scores)\n",
    "\n",
    "# 어텐션 결과 계산\n",
    "attention_output = np.dot(attention_weights, value)\n",
    "\n",
    "print(\"Attention Weights:\", attention_weights)\n",
    "print(\"Dynamic Focus Output:\", attention_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a437ec51-ecd0-40c6-a5ea-84a947a2129a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27a028b0-6739-4507-b822-7a5f9a50fcb8",
   "metadata": {},
   "source": [
    "# Seq2Seq with RNN and Attention (2015) Python 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84659ad6-bf70-40b3-83df-396829d9e5ba",
   "metadata": {},
   "source": [
    "Seq2Seq 모델에 어텐션 메커니즘을 추가한 구조는 2015년 Bahdanau 어텐션으로 잘 알려져 있습니다. \n",
    "\n",
    "이 모델은 기계 번역, 텍스트 생성 등 자연어 처리(NLP)에서 널리 사용됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb5e311-a9b4-41a0-a378-28ba3f34b539",
   "metadata": {},
   "source": [
    "코드 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae74937-b9a3-41c3-9d88-1bcedf7c85f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 메커니즘:\n",
    "# Attention 클래스는 인코더 출력과 디코더의 현재 은닉 상태를 사용해 어텐션 가중치를 계산합니다.\n",
    "# Softmax를 통해 가중치를 정규화하고, 중요한 정보에 더 높은 가중치를 부여합니다.\n",
    "\n",
    "# EncoderRNN:\n",
    "# 입력 시퀀스를 임베딩하고, GRU를 통해 인코딩합니다.\n",
    "# 출력은 인코더의 모든 시퀀스 출력과 마지막 은닉 상태입니다.\n",
    "\n",
    "# DecoderRNN:\n",
    "# 현재 입력과 인코더의 출력, 이전 은닉 상태를 사용해 예측합니다.\n",
    "# 어텐션 메커니즘을 사용하여 인코더의 출력 중 중요한 부분에 집중합니다.\n",
    "\n",
    "# Seq2Seq 모델:\n",
    "# 인코더와 디코더를 결합하여 전체 Seq2Seq 모델을 구성합니다.\n",
    "# 입력 시퀀스를 인코딩한 후, 디코더가 반복적으로 예측을 수행합니다.\n",
    "\n",
    "#         학습 과정:\n",
    "# 임의의 데이터를 생성하고, 모델을 학습시킵니다.\n",
    "# 각 에포크마다 손실 값을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7786eba7-feb9-42b1-89b8-3269a4678c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.6105\n",
      "Epoch 2, Loss: 4.5100\n",
      "Epoch 3, Loss: 4.4259\n",
      "Epoch 4, Loss: 4.3364\n",
      "Epoch 5, Loss: 4.2388\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 디바이스 설정 (GPU 또는 CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 어텐션 메커니즘 정의\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden 차원 조정\n",
    "        hidden = hidden[-1].unsqueeze(1)  # (batch_size, 1, hidden_size)\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "\n",
    "        # hidden을 seq_len만큼 반복\n",
    "        hidden = hidden.repeat(1, seq_len, 1)\n",
    "\n",
    "        # 어텐션 가중치 계산\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        attention = torch.softmax(self.v(energy).squeeze(2), dim=1)\n",
    "\n",
    "        return attention\n",
    "\n",
    "# 인코더 정의\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.embedding(input)\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        return outputs, hidden\n",
    "\n",
    "# 디코더 정의\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        self.attention = Attention(hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # 입력 임베딩\n",
    "        embedded = self.embedding(input).unsqueeze(1)\n",
    "\n",
    "        # 어텐션 가중치 계산\n",
    "        attn_weights = self.attention(hidden, encoder_outputs).unsqueeze(1)\n",
    "\n",
    "        # 컨텍스트 벡터 계산\n",
    "        context = torch.bmm(attn_weights, encoder_outputs)\n",
    "\n",
    "        # GRU 입력 준비\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        # GRU 계산\n",
    "        output, hidden = self.gru(rnn_input, hidden)\n",
    "\n",
    "        # 출력 계산\n",
    "        output = self.fc(torch.cat((output.squeeze(1), context.squeeze(1)), dim=1))\n",
    "\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "# Seq2Seq 모델 정의\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        outputs = torch.zeros(trg.size(0), trg.size(1), self.decoder.output_size).to(device)\n",
    "\n",
    "        input = trg[:, 0]\n",
    "        for t in range(1, trg.size(1)):\n",
    "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs)\n",
    "            outputs[:, t] = output\n",
    "            input = output.argmax(dim=1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "input_size = 100\n",
    "output_size = 100\n",
    "hidden_size = 256\n",
    "\n",
    "# 모델 생성 및 디바이스 이동\n",
    "encoder = EncoderRNN(input_size, hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size, output_size).to(device)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "# 손실 함수와 옵티마이저\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 임의의 입력 데이터 생성\n",
    "src = torch.randint(0, input_size, (32, 10)).to(device)\n",
    "trg = torch.randint(0, output_size, (32, 10)).to(device)\n",
    "\n",
    "# 학습 예시\n",
    "model.train()\n",
    "for epoch in range(1, 6):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(src, trg)\n",
    "    loss = criterion(output.view(-1, output_size), trg.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ec0692-dab0-4dfb-bc85-53030d55fd9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24ba9a6-361f-4c2e-b9c4-80321968fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 값의 의미\n",
    "# 크로스 엔트로피 손실(CrossEntropy Loss)\n",
    "# 이 모델에서는 크로스 엔트로피 손실 함수를 사용했습니다.\n",
    "# 크로스 엔트로피 손실은 모델이 예측한 확률 분포와 실제 레이블의 차이를 측정합니다.\n",
    "# 손실 값이 낮을수록 모델의 예측이 실제 값에 더 가까움을 의미합니다.\n",
    "# b. 손실 값의 감소\n",
    "# 에포크가 진행됨에 따라 손실 값이 점차 감소하고 있습니다:\n",
    "# Epoch 1, Loss: 4.6052에서 Epoch 5, Loss: 4.1920로 감소.\n",
    "# 손실 값이 감소하는 것은 모델이 점점 학습되고 있으며, 입력과 출력 간의 관계를 더 잘 이해하고 있음을 나타냅니다.\n",
    "# 초기 손실 값이 4.6 근처인 것은 출력이 100개의 클래스 중에서 무작위로 예측할 때의 손실 값과 비슷합니다. 이는 초기 예측이 대부분 랜덤임을 의미합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
