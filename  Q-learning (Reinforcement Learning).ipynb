{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c58bcf1f",
   "metadata": {},
   "source": [
    "Q-러닝(Q-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e4129a",
   "metadata": {},
   "source": [
    "# 개요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f61943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 금융 분야에서 강화 학습을 사용하여 최적의 투자 전략이나 포트폴리오 관리를 학습하는 문제는 많이 연구되고 있습니다. \n",
    "# 여기서는 간단한 강화 학습 알고리즘을 사용해 주식 거래 시나리오를 시뮬레이션하는 예시를 파이썬으로 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf8a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Q-러닝(Q-learning) 알고리즘을 사용하여 \n",
    "# 주식 거래 에이전트가 매수(buy), 매도(sell) 또는 **유지(hold)**와 같은 행동을 선택하며, \n",
    "# 포트폴리오의 수익을 극대화하는 전략을 학습하는 방식입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0d2315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 설정\n",
    "# 에이전트는 주식의 가격 변동에 따라 행동을 선택합니다.\n",
    "# 상태는 과거 몇 일간의 주가 데이터를 기반으로 하며, 에이전트는 해당 상태에서 행동을 결정합니다.\n",
    "# 행동은 매수, 매도, 유지의 세 가지 중 하나입니다.\n",
    "# 목표는 최종 수익을 극대화하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5e5e17",
   "metadata": {},
   "source": [
    "# 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c6169f",
   "metadata": {},
   "source": [
    "## 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec2d60b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e12904",
   "metadata": {},
   "source": [
    "## 주식 가격 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eed8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤으로 주식 가격을 시뮬레이션합니다. \n",
    "# 실제 주식 데이터를 사용할 경우, CSV 파일을 읽어서 처리할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8480ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 주식 가격 데이터를 생성 (랜덤)\n",
    "np.random.seed(42)\n",
    "stock_prices = np.random.normal(100, 10, 100)  # 주식 가격 시뮬레이션 (100일간)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f918561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([104.96714153,  98.61735699, 106.47688538, 115.23029856,\n",
       "        97.65846625,  97.65863043, 115.79212816, 107.67434729,\n",
       "        95.30525614, 105.42560044,  95.36582307,  95.34270246,\n",
       "       102.41962272,  80.86719755,  82.75082167,  94.37712471,\n",
       "        89.8716888 , 103.14247333,  90.91975924,  85.87696299,\n",
       "       114.65648769,  97.742237  , 100.67528205,  85.75251814,\n",
       "        94.55617275, 101.1092259 ,  88.49006423, 103.75698018,\n",
       "        93.9936131 ,  97.0830625 ,  93.98293388, 118.52278185,\n",
       "        99.86502775,  89.42289071, 108.22544912,  87.7915635 ,\n",
       "       102.08863595,  80.40329876,  86.71813951, 101.96861236,\n",
       "       107.3846658 , 101.71368281,  98.84351718,  96.98896304,\n",
       "        85.2147801 ,  92.80155792,  95.39361229, 110.57122226,\n",
       "       103.4361829 ,  82.36959845, 103.24083969,  96.1491772 ,\n",
       "        93.23078   , 106.11676289, 110.30999522, 109.31280119,\n",
       "        91.60782477,  96.90787624, 103.31263431, 109.75545127,\n",
       "        95.20825762,  98.14341023,  88.93665026,  88.03793376,\n",
       "       108.12525822, 113.56240029,  99.27989878, 110.03532898,\n",
       "       103.61636025,  93.54880245, 103.61395606, 115.38036566,\n",
       "        99.64173961, 115.64643656,  73.80254896, 108.21902504,\n",
       "       100.87047068,  97.0099265 , 100.91760777,  80.12431085,\n",
       "        97.80328112, 103.57112572, 114.77894045,  94.81729782,\n",
       "        91.91506397,  94.98242956, 109.15402118, 103.2875111 ,\n",
       "        94.70239796, 105.13267433, 100.97077549, 109.68644991,\n",
       "        92.97946906,  96.72337853,  96.07891847,  85.36485052,\n",
       "       102.96120277, 102.61055272, 100.05113457,  97.65412867])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_prices "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d89c33",
   "metadata": {},
   "source": [
    " ## Q-러닝 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a155b652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감마(γ): 미래의 보상을 얼마나 중요하게 여길지를 결정하는 파라미터로, 장기적인 전략을 고려할 때 감마 값을 높게 설정합니다.\n",
    "# 감마 값이 높으면, 먼 미래의 보상까지 중요하게 고려하고, 감마 값이 낮으면 즉각적인 보상에 더 큰 가중치를 둡니다.\n",
    "#예를 들어, 𝛾=0.9일 경우, 에이전트는 미래 보상도 중요하게 여기며 장기적인 전략을 학습하려고 합니다. \n",
    "#반면에 𝛾=0.1이면 에이전트는 즉각적인 보상을 중시하게 됩니다.\n",
    "\n",
    "# 엡실론(ε)\n",
    "#: 탐험과 활용의 균형을 맞추는 파라미터로, 초기에는 탐험을 많이 하고 점차 활용으로 전환해야 합니다.\n",
    "# 엡실론은 에이전트가 탐험(Exploration)과 활용(Exploitation) 사이에서 어느 정도로 탐험을 할지 결정하는 파라미터입니다.\n",
    "# 값의 범위: 0≤𝜖≤1\n",
    "# 역할:\n",
    "\n",
    "# 탐험(Exploration)\n",
    "#: 에이전트가 새로운 행동을 시도하는 것, 즉 Q-테이블에 아직 반영되지 않은 상태와 행동을 탐험하는 것을 말합니다. \n",
    "\n",
    "\n",
    "# 최소 탐험(min_epsilon)\n",
    "# : 학습 후반에도 소량의 탐험을 유지하기 위해 엡실론 값이 감소하는 하한선을 설정합니다.\n",
    "# 엡실론 값이 줄어들 수 있는 최소한의 값을 지정하는 파라미터입니다.\n",
    "# 역할: 학습이 끝나기 전에 엡실론 값이 0에 도달하지 않도록 하여, 학습 후반부에도 소량의 탐험이 남아있도록 설정합니다. \n",
    "# 이로 인해 새로운 상태나 행동을 탐색할 수 있는 여지가 계속 남게 됩니다.\n",
    "# 보통 0.01 또는 0.05 정도로 설정하여, 학습이 끝나갈 때도 소량의 탐험이 계속 유지되도록 합니다.\n",
    "\n",
    "# 학습률(α)\n",
    "# : 새로운 정보를 기존 정보에 얼마나 반영할지를 결정하는 파라미터로, 적절한 값을 설정해야 빠르고 안정적인 학습이 가능합니다.\n",
    "# 0.01≤𝛼≤0.3 사이를 선택하여 균형을 맞추는 것이 좋습니다.\n",
    "\n",
    "# 에피소드 수(episodes)\n",
    "# 에피소드 수는 강화 학습의 학습 과정에서 에이전트가 환경과 상호작용하며 경험을 축적하는 반복 횟수를 의미합니다. \n",
    "# 하나의 에피소드는 초기 상태에서 시작하여 종료 상태에 도달할 때까지의 한 과정입니다.\n",
    "# : 에이전트가 충분히 학습할 수 있는 기회를 제공하며, 에피소드 수가 많을수록 학습의 완성도가 높아집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3293ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-러닝 파라미터 설정\n",
    "gamma = 0.95  # 감가율\n",
    "epsilon = 1.0  # 탐험과 활용 사이의 균형\n",
    "epsilon_decay = 0.995  # 탐험 감소 속도\n",
    "min_epsilon = 0.01  # 최소 탐험\n",
    "learning_rate = 0.01  # 학습률\n",
    "num_episodes = 1000  # 에피소드 수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864d8b40",
   "metadata": {},
   "source": [
    "## 행동 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a39d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행동 정의 (0: 유지, 1: 매수, 2: 매도)\n",
    "actions = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84824fb",
   "metadata": {},
   "source": [
    "## Q-테이블 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3749c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-테이블 초기화:\n",
    "# 각 상태에서의 행동에 대한 Q-값을 저장할 테이블입니다. \n",
    "# 상태는 시간 단계로 구성되며, 각 시간 단계에서 에이전트는 매수, 매도, 유지 중 하나의 행동을 선택합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28fa9974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-테이블 초기화 (상태 x 행동)\n",
    "Q_table = np.zeros((len(stock_prices), len(actions)))\n",
    "\n",
    "# 에이전트의 초기 상태 (주식 보유 여부, 현금, 포트폴리오 가치) 초기금액 : 10000\n",
    "initial_cash = 10000\n",
    "initial_stock_held = 0\n",
    "initial_portfolio_value = initial_cash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f4aaae",
   "metadata": {},
   "source": [
    "## 강화학습 환경 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25d4193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_reward(cash, stock_held, portfolio_value, current_price, previous_price):\n",
    "    # 포트폴리오의 가치가 증가했으면 보상 증가\n",
    "    new_portfolio_value = cash + stock_held * current_price\n",
    "    reward = new_portfolio_value - portfolio_value\n",
    "    return reward, new_portfolio_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06977104",
   "metadata": {},
   "source": [
    "## Q-러닝 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b34fd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 에피소드마다 주어진 가격 데이터에 따라 에이전트가 주식을 사고 팔며, 보상을 최대화하도록 Q-값을 업데이트합니다.\n",
    "# 에이전트는 주식을 매수(buy), 매도(sell) 또는 **유지(hold)**하는 세 가지 행동 중 하나를 선택합니다.\n",
    "# 에이전트는 미래의 보상을 고려하며 행동을 선택하고, 각 행동에 따른 포트폴리오 가치의 변화를 학습합니다.\n",
    "\n",
    "# 탐험과 활용:\n",
    "# 학습 초기에는 에이전트가 탐험(랜덤 행동)을 많이 하지만, 학습이 진행될수록 최적의 행동을 선택하는 확률이 증가합니다 (탐험-활용 균형).\n",
    "\n",
    "# 보상 함수:\n",
    "# 보상은 에이전트의 포트폴리오 가치가 얼마나 증가했는지를 기반으로 합니다. 포트폴리오 가치가 증가하면 긍정적인 보상을 받고, 감소하면 음의 보상을 받습니다.\n",
    "# Q-값 업데이트:\n",
    "\n",
    "# 벨만 방정식을 기반으로 Q-값을 업데이트하며, 이 과정에서 학습률과 감가율을 고려합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1272879c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-테이블:\n",
      "[[  5.59764794   2.22591851  32.83946911]\n",
      " [  6.76402517  36.72362856   4.32446512]\n",
      " [  3.8375748   32.49534441  -0.09167433]\n",
      " [ -6.91918691 -14.07953943  17.9697408 ]\n",
      " [ 39.80305362   8.06270898   9.15994982]\n",
      " [  7.70483387  45.07764635   2.56993649]\n",
      " [ -5.80566782 -10.71109234  13.06714072]\n",
      " [ -3.51595355  -9.64811491  26.84630094]\n",
      " [  4.02845942  33.22907543   3.17340719]\n",
      " [ -5.75750503 -11.42536308  29.21849515]\n",
      " [ 35.56115875   3.07801044   3.93805448]\n",
      " [  3.04193293  41.42653768   1.72663599]\n",
      " [-13.84759415 -26.16883527  39.85403976]\n",
      " [ 45.46129106   9.36720435  11.01162969]\n",
      " [ 12.27926054  50.45686533   9.24733256]\n",
      " [  4.8063748    4.38842047  43.44758843]\n",
      " [  9.80460219  49.25212163   6.43722994]\n",
      " [ -7.65468307 -11.66547676  41.20681524]\n",
      " [ 10.35643757   7.00536451  48.25823762]\n",
      " [ 23.69458287  55.58940559   9.77138096]\n",
      " [-18.35270343 -24.79878496  31.99427013]\n",
      " [  2.56715027  39.12470923   2.26255508]\n",
      " [ -8.22683746 -17.1212481   42.54869639]\n",
      " [ 14.24568538  49.9104664    9.03175192]\n",
      " [ 46.62804818  10.64590218   6.89842391]\n",
      " [ -5.61186049 -12.93298538  45.45274474]\n",
      " [ 19.79152729  52.1709574   11.13686668]\n",
      " [ -4.22083035  -6.26225869  41.75592105]\n",
      " [ 12.02788324  49.54452744  10.56411895]\n",
      " [ 12.61232749  10.02906324  54.10716087]\n",
      " [ 16.56319341  63.39609938  12.42680326]\n",
      " [-23.17449192 -33.50098752  42.89495794]\n",
      " [ -0.39972194  -7.46406783  51.73598455]\n",
      " [ 20.49320012  58.95250466  11.18170359]\n",
      " [-12.84718637 -22.04414227  44.9937115 ]\n",
      " [ 11.9444311   52.10280953   4.87773272]\n",
      " [-17.0095643  -21.03314513  42.91550367]\n",
      " [ 16.64397388  49.77563849  16.16357478]\n",
      " [ 25.3074745   48.80618366  19.7452543 ]\n",
      " [  2.94020958  21.67007933   1.58847264]\n",
      " [-13.16731867 -14.1383333    8.51964506]\n",
      " [ -5.07762548  -5.27530942  24.28930381]\n",
      " [ -2.61292388  -2.84165811  32.19535809]\n",
      " [ -5.17647848  -9.04961124  37.79618268]\n",
      " [ 15.68322929  44.04647443  10.91968629]\n",
      " [ 11.81297404  13.85843862  41.66701891]\n",
      " [ 18.05074585  46.83482151   7.53448051]\n",
      " [-11.23307643 -13.14461494  35.14166371]\n",
      " [ -5.78182493 -21.91332561  39.05621032]\n",
      " [ 23.52418051  42.53383557  13.11620345]\n",
      " [ -3.0106296   -5.92470071  24.14382772]\n",
      " [  9.71159896   6.16766937  27.11966785]\n",
      " [ 17.0304712   30.32917086   7.85243011]\n",
      " [  3.60535737  19.75395089   1.50684043]\n",
      " [ -7.2637589   -7.47721479  13.86024319]\n",
      " [-17.45093155 -29.50208171  17.99610922]\n",
      " [ 22.406742    13.01360746  10.79918591]\n",
      " [ 10.35728756  26.06783144   7.81434904]\n",
      " [  0.53695718  22.835597    -1.63588045]\n",
      " [-23.72654589 -27.83021339  12.03794098]\n",
      " [  3.15025919  29.80794034   1.22633152]\n",
      " [ -7.89261175 -11.25006272  26.47960404]\n",
      " [ 16.3180569   16.02945319  39.74774217]\n",
      " [ 34.9447247   44.16185466  20.57575062]\n",
      " [  2.43385515  26.82643372   1.490342  ]\n",
      " [-18.57459071 -25.36958128  19.16699361]\n",
      " [ 12.92738572  38.09011605  11.62479552]\n",
      " [-11.14971334 -12.8137988   17.28498984]\n",
      " [ -4.23253949 -10.67685482  26.27764949]\n",
      " [ 18.2573069   29.79449266  21.78342514]\n",
      " [ 10.46684913  21.86820926   4.97561361]\n",
      " [  0.66133087 -20.84413481 -12.41137602]\n",
      " [ 15.45976993  39.87419459   5.01383718]\n",
      " [-45.88359538 -63.01923901  -5.86613496]\n",
      " [ 90.88728609  54.36244371  30.28012517]\n",
      " [-12.01492921 -13.47611531  25.74583601]\n",
      " [ -2.53805026  -4.4844451   41.62517081]\n",
      " [  3.75480069  48.98972219   2.55093526]\n",
      " [-10.30576261 -24.43084815  49.71914302]\n",
      " [ 37.58631514  55.63971049  21.89518955]\n",
      " [ 14.03851178  37.80508975  11.45132853]\n",
      " [  4.49855387  28.41928188   4.42883813]\n",
      " [-32.66491919 -49.75217793  -4.5841473 ]\n",
      " [  8.51549455   7.43870937  41.22912268]\n",
      " [ 18.38568211  47.63354123  16.21943728]\n",
      " [ 14.79938042  44.68170553  13.988662  ]\n",
      " [-13.88507585 -14.61678255   2.33387195]\n",
      " [ -6.5967807  -11.688185    17.06087717]\n",
      " [ 15.5935801   28.77855524  10.44934535]\n",
      " [  7.73455778  -5.76174444  -1.63772789]\n",
      " [  5.13002074  18.81288309   2.70154048]\n",
      " [-28.54039914 -34.46711467  -7.46353429]\n",
      " [  6.95421896  30.30059619   6.72277061]\n",
      " [  1.62560315   0.72372451  19.61027932]\n",
      " [ -1.51721176  -9.32994754  21.32210415]\n",
      " [ 30.4641019   34.56183704  16.89214638]\n",
      " [ -5.71798125  -5.65366371  -3.08316061]\n",
      " [ -7.49777722  -7.73331829  -1.60381654]\n",
      " [ -4.42224288  -5.54155168  -0.35830977]\n",
      " [  0.           0.           0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Q-러닝 알고리즘\n",
    "for episode in range(num_episodes):\n",
    "    cash = initial_cash\n",
    "    stock_held = initial_stock_held\n",
    "    portfolio_value = initial_portfolio_value\n",
    "    state = 0  # 첫 번째 상태부터 시작\n",
    "\n",
    "    for t in range(len(stock_prices) - 1):\n",
    "        current_price = stock_prices[state]\n",
    "        next_price = stock_prices[state + 1]\n",
    "\n",
    "        # 탐험(Exploration) 또는 활용(Exploitation) 결정\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice(actions)  # 랜덤 선택 (탐험)\n",
    "        else:\n",
    "            action = np.argmax(Q_table[state])  # 최적 행동 선택 (활용)\n",
    "\n",
    "        # 행동에 따른 상태 변화\n",
    "        if action == 1 and cash >= current_price:  # 매수\n",
    "            stock_held += 1\n",
    "            cash -= current_price\n",
    "        elif action == 2 and stock_held > 0:  # 매도\n",
    "            stock_held -= 1\n",
    "            cash += current_price\n",
    "\n",
    "        # 보상 계산\n",
    "        reward, new_portfolio_value = get_reward(cash, stock_held, portfolio_value, next_price, current_price)\n",
    "\n",
    "        # Q-값 업데이트 (벨만 방정식)\n",
    "        next_state = state + 1\n",
    "        Q_table[state, action] = Q_table[state, action] + learning_rate * (\n",
    "            reward + gamma * np.max(Q_table[next_state]) - Q_table[state, action]\n",
    "        )\n",
    "\n",
    "        # 상태 업데이트\n",
    "        state = next_state\n",
    "        portfolio_value = new_portfolio_value\n",
    "\n",
    "    # 탐험-활용 균형 조절 (epsilon 감소)\n",
    "    if epsilon > min_epsilon:\n",
    "        epsilon *= epsilon_decay\n",
    "# 학습 후의 Q-테이블 확인\n",
    "print(\"Q-테이블:\")\n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610b6216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b6cf089",
   "metadata": {},
   "source": [
    "## 학습 후 정책 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2a90b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 완료된 후, 학습된 Q-테이블을 사용해 최적의 정책을 실행합니다. \n",
    "# 에이전트는 학습한 대로 주식을 사고 팔며 포트폴리오 가치를 극대화하려 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9f110e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "날짜 1: 매수, 주가: 98.62\n",
      "날짜 2: 매수, 주가: 106.48\n",
      "날짜 3: 매도, 주가: 115.23\n",
      "날짜 5: 매수, 주가: 97.66\n",
      "날짜 6: 매도, 주가: 115.79\n",
      "날짜 7: 매도, 주가: 107.67\n",
      "날짜 8: 매수, 주가: 95.31\n",
      "날짜 9: 매도, 주가: 105.43\n",
      "날짜 11: 매수, 주가: 95.34\n",
      "날짜 12: 매도, 주가: 102.42\n",
      "날짜 14: 매수, 주가: 82.75\n",
      "날짜 15: 매도, 주가: 94.38\n",
      "날짜 16: 매수, 주가: 89.87\n",
      "날짜 17: 매도, 주가: 103.14\n",
      "날짜 19: 매수, 주가: 85.88\n",
      "날짜 20: 매도, 주가: 114.66\n",
      "날짜 21: 매수, 주가: 97.74\n",
      "날짜 22: 매도, 주가: 100.68\n",
      "날짜 23: 매수, 주가: 85.75\n",
      "날짜 25: 매도, 주가: 101.11\n",
      "날짜 26: 매수, 주가: 88.49\n",
      "날짜 27: 매도, 주가: 103.76\n",
      "날짜 28: 매수, 주가: 93.99\n",
      "날짜 29: 매도, 주가: 97.08\n",
      "날짜 30: 매수, 주가: 93.98\n",
      "날짜 31: 매도, 주가: 118.52\n",
      "날짜 33: 매수, 주가: 89.42\n",
      "날짜 34: 매도, 주가: 108.23\n",
      "날짜 35: 매수, 주가: 87.79\n",
      "날짜 36: 매도, 주가: 102.09\n",
      "날짜 37: 매수, 주가: 80.40\n",
      "날짜 38: 매수, 주가: 86.72\n",
      "날짜 39: 매수, 주가: 101.97\n",
      "날짜 40: 매도, 주가: 107.38\n",
      "날짜 41: 매도, 주가: 101.71\n",
      "날짜 42: 매도, 주가: 98.84\n",
      "날짜 44: 매수, 주가: 85.21\n",
      "날짜 45: 매도, 주가: 92.80\n",
      "날짜 46: 매수, 주가: 95.39\n",
      "날짜 47: 매도, 주가: 110.57\n",
      "날짜 49: 매수, 주가: 82.37\n",
      "날짜 50: 매도, 주가: 103.24\n",
      "날짜 52: 매수, 주가: 93.23\n",
      "날짜 53: 매수, 주가: 106.12\n",
      "날짜 54: 매도, 주가: 110.31\n",
      "날짜 55: 매도, 주가: 109.31\n",
      "날짜 57: 매수, 주가: 96.91\n",
      "날짜 58: 매수, 주가: 103.31\n",
      "날짜 59: 매도, 주가: 109.76\n",
      "날짜 60: 매수, 주가: 95.21\n",
      "날짜 61: 매도, 주가: 98.14\n",
      "날짜 62: 매도, 주가: 88.94\n",
      "날짜 63: 매수, 주가: 88.04\n",
      "날짜 64: 매수, 주가: 108.13\n",
      "날짜 65: 매도, 주가: 113.56\n",
      "날짜 66: 매수, 주가: 99.28\n",
      "날짜 67: 매도, 주가: 110.04\n",
      "날짜 68: 매도, 주가: 103.62\n",
      "날짜 69: 매수, 주가: 93.55\n",
      "날짜 70: 매수, 주가: 103.61\n",
      "날짜 72: 매수, 주가: 99.64\n",
      "날짜 73: 매도, 주가: 115.65\n",
      "날짜 75: 매도, 주가: 108.22\n",
      "날짜 76: 매도, 주가: 100.87\n",
      "날짜 77: 매수, 주가: 97.01\n",
      "날짜 78: 매도, 주가: 100.92\n",
      "날짜 79: 매수, 주가: 80.12\n",
      "날짜 80: 매수, 주가: 97.80\n",
      "날짜 81: 매수, 주가: 103.57\n",
      "날짜 82: 매도, 주가: 114.78\n",
      "날짜 83: 매도, 주가: 94.82\n",
      "날짜 84: 매수, 주가: 91.92\n",
      "날짜 85: 매수, 주가: 94.98\n",
      "날짜 86: 매도, 주가: 109.15\n",
      "날짜 87: 매도, 주가: 103.29\n",
      "날짜 88: 매수, 주가: 94.70\n",
      "날짜 90: 매수, 주가: 100.97\n",
      "날짜 91: 매도, 주가: 109.69\n",
      "날짜 92: 매수, 주가: 92.98\n",
      "날짜 93: 매도, 주가: 96.72\n",
      "날짜 94: 매도, 주가: 96.08\n",
      "날짜 95: 매수, 주가: 85.36\n",
      "날짜 96: 매도, 주가: 102.96\n",
      "날짜 97: 매도, 주가: 102.61\n",
      "최종 포트폴리오 가치: 10456.57\n"
     ]
    }
   ],
   "source": [
    "# 학습 후 최적 정책 테스트\n",
    "cash = initial_cash\n",
    "stock_held = initial_stock_held\n",
    "portfolio_value = initial_portfolio_value\n",
    "state = 0\n",
    "\n",
    "for t in range(len(stock_prices) - 1):\n",
    "    current_price = stock_prices[state]\n",
    "    action = np.argmax(Q_table[state])  # 최적 행동 선택\n",
    "    if action == 1 and cash >= current_price:  # 매수\n",
    "        stock_held += 1\n",
    "        cash -= current_price\n",
    "        print(f\"날짜 {t}: 매수, 주가: {current_price:.2f}\")\n",
    "    elif action == 2 and stock_held > 0:  # 매도\n",
    "        stock_held -= 1\n",
    "        cash += current_price\n",
    "        print(f\"날짜 {t}: 매도, 주가: {current_price:.2f}\")\n",
    "    state += 1\n",
    "\n",
    "final_portfolio_value = cash + stock_held * stock_prices[-1]\n",
    "print(f\"최종 포트폴리오 가치: {final_portfolio_value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c836fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과\n",
    "# 에이전트는 주어진 주식 가격 데이터를 통해 최적의 거래 전략을 학습합니다.\n",
    "# 최종적으로, 에이전트의 포트폴리오 가치가 얼마나 증가했는지 확인할 수 있습니다.\n",
    "\n",
    "# 초기금액 : 10000==> 최종 포트폴리오 가치: 10456.57"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2389d342",
   "metadata": {},
   "source": [
    "# 코드 종합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916c138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# 간단한 주식 가격 데이터를 생성 (랜덤)\n",
    "np.random.seed(42)\n",
    "stock_prices = np.random.normal(100, 10, 100)  # 주식 가격 시뮬레이션 (100일간)\n",
    "\n",
    "# Q-러닝 파라미터 설정\n",
    "gamma = 0.95  # 감가율\n",
    "epsilon = 1.0  # 탐험과 활용 사이의 균형\n",
    "epsilon_decay = 0.995  # 탐험 감소 속도\n",
    "min_epsilon = 0.01  # 최소 탐험\n",
    "learning_rate = 0.01  # 학습률\n",
    "num_episodes = 1000  # 에피소드 수\n",
    "\n",
    "# 행동 정의 (0: 유지, 1: 매수, 2: 매도)\n",
    "actions = [0, 1, 2]\n",
    "\n",
    "# Q-테이블 초기화 (상태 x 행동)\n",
    "Q_table = np.zeros((len(stock_prices), len(actions)))\n",
    "\n",
    "# 에이전트의 초기 상태 (주식 보유 여부, 현금, 포트폴리오 가치)\n",
    "initial_cash = 10000\n",
    "initial_stock_held = 0\n",
    "initial_portfolio_value = initial_cash\n",
    "\n",
    "# 강화학습 환경 정의\n",
    "def get_reward(cash, stock_held, portfolio_value, current_price, previous_price):\n",
    "    # 포트폴리오의 가치가 증가했으면 보상 증가\n",
    "    new_portfolio_value = cash + stock_held * current_price\n",
    "    reward = new_portfolio_value - portfolio_value\n",
    "    return reward, new_portfolio_value\n",
    "\n",
    "# Q-러닝 알고리즘\n",
    "for episode in range(num_episodes):\n",
    "    cash = initial_cash\n",
    "    stock_held = initial_stock_held\n",
    "    portfolio_value = initial_portfolio_value\n",
    "    state = 0  # 첫 번째 상태부터 시작\n",
    "\n",
    "    for t in range(len(stock_prices) - 1):\n",
    "        current_price = stock_prices[state]\n",
    "        next_price = stock_prices[state + 1]\n",
    "\n",
    "        # 탐험(Exploration) 또는 활용(Exploitation) 결정\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice(actions)  # 랜덤 선택 (탐험)\n",
    "        else:\n",
    "            action = np.argmax(Q_table[state])  # 최적 행동 선택 (활용)\n",
    "\n",
    "        # 행동에 따른 상태 변화\n",
    "        if action == 1 and cash >= current_price:  # 매수\n",
    "            stock_held += 1\n",
    "            cash -= current_price\n",
    "        elif action == 2 and stock_held > 0:  # 매도\n",
    "            stock_held -= 1\n",
    "            cash += current_price\n",
    "\n",
    "        # 보상 계산\n",
    "        reward, new_portfolio_value = get_reward(cash, stock_held, portfolio_value, next_price, current_price)\n",
    "\n",
    "        # Q-값 업데이트 (벨만 방정식)\n",
    "        next_state = state + 1\n",
    "        Q_table[state, action] = Q_table[state, action] + learning_rate * (\n",
    "            reward + gamma * np.max(Q_table[next_state]) - Q_table[state, action]\n",
    "        )\n",
    "\n",
    "        # 상태 업데이트\n",
    "        state = next_state\n",
    "        portfolio_value = new_portfolio_value\n",
    "\n",
    "    # 탐험-활용 균형 조절 (epsilon 감소)\n",
    "    if epsilon > min_epsilon:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "# 학습 후의 Q-테이블 확인\n",
    "print(\"Q-테이블:\")\n",
    "print(Q_table)\n",
    "\n",
    "# 학습 후 최적 정책 테스트\n",
    "cash = initial_cash\n",
    "stock_held = initial_stock_held\n",
    "portfolio_value = initial_portfolio_value\n",
    "state = 0\n",
    "\n",
    "for t in range(len(stock_prices) - 1):\n",
    "    current_price = stock_prices[state]\n",
    "    action = np.argmax(Q_table[state])  # 최적 행동 선택\n",
    "    if action == 1 and cash >= current_price:  # 매수\n",
    "        stock_held += 1\n",
    "        cash -= current_price\n",
    "        print(f\"날짜 {t}: 매수, 주가: {current_price:.2f}\")\n",
    "    elif action == 2 and stock_held > 0:  # 매도\n",
    "        stock_held -= 1\n",
    "        cash += current_price\n",
    "        print(f\"날짜 {t}: 매도, 주가: {current_price:.2f}\")\n",
    "    state += 1\n",
    "\n",
    "final_portfolio_value = cash + stock_held * stock_prices[-1]\n",
    "print(f\"최종 포트폴리오 가치: {final_portfolio_value:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
