{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a09835a",
   "metadata": {},
   "source": [
    "TensorFlow와 Keras를 사용하여 MNIST 손글씨 숫자 데이터셋에 대해 간단한 인공 신경망(ANN) 모델을 구축하고 학습하는 예시입니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90aa0d0",
   "metadata": {},
   "source": [
    "# 데이터 셋 소개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acfc4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 손글씨 숫자 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab3faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  MNIST 데이터셋은 손으로 쓴 숫자 이미지로 구성되어 있으며, 0부터 9까지의 숫자를 포함하고 있습니다.\n",
    "\n",
    "# MNIST 데이터셋의 주요 특징\n",
    "# 샘플 수:\n",
    "\n",
    "# 훈련 데이터: 60,000개의 손글씨 숫자 이미지\n",
    "# 테스트 데이터: 10,000개의 손글씨 숫자 이미지\n",
    "# 총 70,000개의 이미지로 구성되어 있습니다.\n",
    "# 이미지 크기:\n",
    "\n",
    "# 각 이미지의 크기는 28x28 픽셀입니다.\n",
    "# 이미지의 해상도는 낮고 흑백입니다. 픽셀 값은 0에서 255까지의 값으로, 0은 검정색(배경), 255는 흰색(숫자)에 해당합니다.\n",
    "# 레이블:\n",
    "\n",
    "# 각 이미지에 대응되는 레이블이 존재하며, 이 레이블은 0부터 9까지의 숫자를 나타냅니다.\n",
    "# 레이블은 이미지에 나타난 손글씨 숫자를 의미하며, 분류 문제의 목표는 이미지를 보고 숫자를 예측하는 것입니다.\n",
    "# 이미지 데이터 형식:\n",
    "\n",
    "# 각 이미지는 28x28 픽셀 크기의 2D 배열로 표현되며, 각 픽셀은 0~255 사이의 그레이스케일 값을 가집니다.\n",
    "# 예를 들어, 0은 완전한 검정색, 255는 완전한 흰색을 나타냅니다.\n",
    "# 데이터셋 구조\n",
    "# x_train, y_train: 훈련 데이터와 훈련 레이블\n",
    "# x_train: (60,000, 28, 28) 크기의 배열로, 60,000개의 이미지와 각 이미지의 28x28 픽셀을 포함합니다.\n",
    "# y_train: 각 이미지에 대응되는 레이블(0~9)\n",
    "# x_test, y_test: 테스트 데이터와 테스트 레이블\n",
    "# x_test: (10,000, 28, 28) 크기의 배열로, 10,000개의 테스트 이미지\n",
    "# y_test: 각 테스트 이미지에 대응되는 레이블\n",
    "# 데이터셋 활용\n",
    "# MNIST 데이터셋은 머신러닝 및 딥러닝 알고리즘을 테스트하고 비교하는 데 많이 사용됩니다. 특히, 다음과 같은 모델에 자주 활용됩니다.\n",
    "\n",
    "# 인공신경망(ANN): 간단한 신경망을 이용해 손글씨 숫자를 분류하는 문제로 많이 사용됩니다.\n",
    "# 컨볼루션 신경망(CNN): 이미지 데이터 특성에 맞춰 CNN을 적용하여 더 높은 정확도를 얻을 수 있습니다.\n",
    "# 기본적인 머신러닝 알고리즘: SVM, KNN, 랜덤 포레스트 등 전통적인 머신러닝 알고리즘으로도 MNIST 분류 문제를 해결할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7fd98c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MNIST 데이터셋 로드\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 첫 번째 이미지 출력\n",
    "plt.imshow(x_train[0], cmap='gray')\n",
    "plt.title(f\"Label: {y_train[0]}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8274c73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 데이터셋에서 첫 번째 이미지를 출력하고 그에 해당하는 레이블을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3049ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 데이터셋은 이미지 분류 문제에서 널리 사용되는 데이터셋으로, 초보자가 딥러닝과 머신러닝을 학습하기에 이상적입니다. \n",
    "# 픽셀 데이터를 이용한 손글씨 숫자 분류는 다양한 모델의 성능을 비교하고 학습하는 데 매우 유용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a845b47",
   "metadata": {},
   "source": [
    "# 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52d552f",
   "metadata": {},
   "source": [
    "## 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28784951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 데이터셋 로드: tf.keras.datasets.mnist를 통해 MNIST 데이터를 로드합니다. \n",
    "# 이 데이터셋은 28x28 크기의 손글씨 숫자 이미지와 각 이미지에 대한 레이블(0~9)이 포함되어 있습니다.\n",
    "# 정규화(Normalization): 픽셀 값이 0~255 사이의 값이기 때문에, 이를 0~1 사이로 스케일링하여 학습이 잘 이루어지도록 정규화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26966b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b8d5148",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # 정규화\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56988f4",
   "metadata": {},
   "source": [
    " ## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65406543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential 모델: Sequential()을 통해 각 레이어가 순차적으로 연결되는 신경망 모델을 정의합니다.\n",
    "# Flatten 층: 입력 데이터가 28x28 이미지이므로 이를 일차원 벡터로 펼쳐줍니다.\n",
    "# Dense 층: 첫 번째 은닉층으로 128개의 뉴런을 가지며, 활성화 함수로 ReLU를 사용합니다.\n",
    "# Dense 층을 이용한 다중 클래스 분류 문제를 해결합니다.\n",
    "# Dropout 층: 과적합을 방지하기 위해 20%의 뉴런을 무작위로 끕니다.\n",
    "# Dense 출력층: 10개의 뉴런을 가지는 출력층입니다. 소프트맥스(Softmax) 활성화 함수를 사용하여 다중 클래스 분류를 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d3bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구조:\n",
    "# 입력: 28x28 픽셀 크기의 이미지를 펼친 후,\n",
    "# 128개의 은닉 뉴런을 거쳐,\n",
    "# 출력층에서 10개의 클래스(숫자 0~9)에 대한 확률을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eafe7900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),  # 입력층\n",
    "    layers.Dense(128, activation='relu'),  # 은닉층\n",
    "    layers.Dropout(0.2),                   # 드롭아웃\n",
    "    layers.Dense(10, activation='softmax') # 출력층\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9db0c91",
   "metadata": {},
   "source": [
    "## 모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1b3099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer: Adam 옵티마이저를 사용하여 가중치 업데이트를 수행합니다.\n",
    "# Loss 함수: 다중 클래스 분류 문제이므로 sparse_categorical_crossentropy 손실 함수를 사용합니다. 이는 레이블이 정수로 제공될 때 사용됩니다.\n",
    "# 평가지표: 모델 성능을 평가하기 위한 지표로 accuracy를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96a8d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d8bdd8",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9162a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습: 훈련 데이터를 사용해 5번 반복(에포크)하여 모델을 학습시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4de2ed0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8582 - loss: 0.4873\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9548 - loss: 0.1502\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9671 - loss: 0.1070\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9752 - loss: 0.0838\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9783 - loss: 0.0692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2224195a910>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b819a25",
   "metadata": {},
   "source": [
    "## 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e87bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가: 테스트 데이터를 사용해 학습된 모델을 평가하고 정확도와 손실 값을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c56356e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9722 - loss: 0.0916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07774345576763153, 0.9758999943733215]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c437ec2",
   "metadata": {},
   "source": [
    "## 결과 해석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1e2de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가 값으로 제공된 두 개의 값 **0.07774345576763153**와 **0.9758999943733215**는 각각 **손실 값(loss)**과 **정확도(accuracy)**를 의미합니다. 이를 설명하겠습니다:\n",
    "\n",
    "# 1. 손실 값 (Loss): 0.07774345576763153\n",
    "# 손실(Loss)은 모델이 예측한 값과 실제 정답 사이의 차이를 수치화한 값입니다. \n",
    "# 이 값은 모델이 얼마나 잘못 예측했는지를 나타냅니다.\n",
    "# 손실 함수로는 주로 크로스 엔트로피(Categorical Crossentropy)를 사용합니다. 이 값은 낮을수록 모델의 성능이 더 좋다는 의미입니다. 즉, 손실 값이 작으면 모델의 예측이 실제 값에 더 가깝다는 것을 의미합니다.\n",
    "# 0.0777의 손실 값은 상당히 작은 값으로, 이는 모델이 예측하는 값이 실제 정답과 매우 가까운 결과를 내고 있음을 의미합니다.\n",
    "\n",
    "# 2. 정확도 (Accuracy): 0.9758999943733215\n",
    "# 정확도(Accuracy)는 모델이 얼마나 정확하게 예측했는지를 나타내는 값으로, 예측한 값 중에서 실제 정답과 일치하는 비율입니다.\n",
    "# 0.9759는 정확도가 97.59%임을 의미합니다. 즉, 테스트 데이터에 대해 모델이 97.59%의 정확도로 올바른 예측을 했다는 의미입니다.\n",
    "# 높은 정확도는 모델이 테스트 데이터에서 상당히 좋은 성능을 발휘하고 있음을 나타냅니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745a878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종합 설명:\n",
    "# 손실 값이 0.0777로 낮고, 정확도가 97.59%로 매우 높은 것으로 보아, 이 모델은 주어진 데이터셋에 대해 매우 좋은 성능을 보이고 있다고 해석할 수 있습니다.\n",
    "# 모델의 목표는 손실 값을 최소화하고 정확도를 최대화하는 것이므로, 이 평가 결과는 훈련된 모델이 테스트 데이터에서도 매우 좋은 성능을 발휘하고 있음을 의미합니다.\n",
    "\n",
    "# 참고:\n",
    "# 손실 값은 주로 학습 과정에서 모델의 성능을 평가하는 지표이며, 낮을수록 모델이 더 잘 학습한 것입니다.\n",
    "# 정확도는 모델이 예측한 정답의 비율을 나타내는 지표로, 분류 문제에서는 중요한 성능 지표입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac9a1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "610ae0e9",
   "metadata": {},
   "source": [
    "## 종합 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ecd2f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.8577 - loss: 0.4837\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9538 - loss: 0.1539\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9682 - loss: 0.1093\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9728 - loss: 0.0897\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9771 - loss: 0.0740\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9760 - loss: 0.0819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07233167439699173, 0.978600025177002]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# 1. 데이터 준비\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # 정규화\n",
    "\n",
    "# 2. 모델 정의\n",
    "model = models.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),  # 입력층\n",
    "    layers.Dense(128, activation='relu'),  # 은닉층\n",
    "    layers.Dropout(0.2),                   # 드롭아웃\n",
    "    layers.Dense(10, activation='softmax') # 출력층\n",
    "])\n",
    "\n",
    "# 3. 모델 컴파일\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "# 5. 모델 평가\n",
    "model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bf5b64",
   "metadata": {},
   "source": [
    "# 드롭아웃(Dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9142fa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow와 Keras에서 **드롭아웃(Dropout)**을 사용하는 간단한 신경망 모델의 예시 코드입니다. \n",
    "# 드롭아웃은 과적합을 방지하기 위해 뉴런의 일부를 랜덤하게 비활성화하는 기술\n",
    "# 모델이 학습하는 동안 무작위로 뉴런을 드롭(drop)하여 특정 뉴런에 너무 의존하지 않도록 만듭니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aefbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 드롭아웃이 중요한 이유:\n",
    "\n",
    "# 과적합 방지: 드롭아웃은 모델이 학습할 때 특정 뉴런에 의존하지 않도록 하여, \n",
    "# 모델이 훈련 데이터에 과도하게 적합(과적합)되는 것을 방지합니다.\n",
    "# 일반화 성능 향상: 드롭아웃은 모델의 일반화 성능을 향상시켜 새로운 데이터에서도 더 좋은 성능을 발휘할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800701ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 설명:\n",
    "# 데이터 준비: MNIST 손글씨 숫자 데이터셋을 로드하고, 각 픽셀 값을 0~1 사이로 정규화합니다.\n",
    "# 모델 정의:\n",
    "# Flatten: 입력 이미지를 1차원 벡터로 변환합니다.\n",
    "# Dense(128, activation='relu'): 128개의 뉴런을 가진 은닉층을 정의하며, 활성화 함수로 ReLU를 사용합니다.\n",
    "# Dropout(0.5): 드롭아웃을 적용해 은닉층의 50% 뉴런을 학습 중 무작위로 비활성화합니다.\n",
    "# Dense(10, activation='softmax'): 10개의 뉴런을 가진 출력층을 정의하며, softmax 활성화 함수로 다중 클래스 분류를 수행합니다.\n",
    "# 컴파일: Adam 옵티마이저와 sparse_categorical_crossentropy 손실 함수를 사용하여 모델을 컴파일합니다.\n",
    "# 모델 학습: 훈련 데이터로 5번의 에포크 동안 모델을 학습시킵니다.\n",
    "# 모델 평가: 테스트 데이터로 모델을 평가하고 정확도를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# 1. 데이터 준비 (MNIST 손글씨 숫자 데이터셋)\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 2. 데이터 전처리 (0~255 사이의 값을 0~1 사이로 정규화)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# 3. 모델 정의 (드롭아웃 적용)\n",
    "model = models.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),       # 입력층 (28x28 이미지를 펼침)\n",
    "    layers.Dense(128, activation='relu'),       # 은닉층 (128개의 뉴런)\n",
    "    layers.Dropout(0.5),                        # 드롭아웃 (50%의 뉴런을 드롭)\n",
    "    layers.Dense(10, activation='softmax')      # 출력층 (10개의 뉴런, 0~9까지의 숫자 분류)\n",
    "])\n",
    "\n",
    "# 4. 모델 컴파일 (손실 함수와 옵티마이저 정의)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 5. 모델 학습 (5번의 에포크 동안 학습)\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "# 6. 모델 평가 (테스트 데이터로 모델 성능 평가)\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'테스트 정확도: {test_acc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6eee14",
   "metadata": {},
   "source": [
    "# PyTorch 사례"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa48311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch는 **Facebook AI Research(FAIR)**에서 개발한 오픈 소스 딥러닝 라이브러리로, 주로 딥러닝 연구와 개발에 사용됩니다. PyTorch는 특히 자연어 처리(NLP), 컴퓨터 비전, 강화 학습 등의 다양한 딥러닝 분야에서 널리 사용되고 있습니다. TensorFlow와 함께 가장 인기 있는 딥러닝 프레임워크 중 하나입니다.\n",
    "\n",
    "# PyTorch의 주요 특징\n",
    "# 동적 계산 그래프(Autograd):\n",
    "\n",
    "# PyTorch는 동적 계산 그래프를 지원하는데, 이는 그래프를 실시간으로 정의하고 변경할 수 있는 기능을 의미합니다. 즉, 모델이 실행되는 동안 계산 그래프가 생성되며, 복잡한 계산이 실시간으로 수행됩니다.\n",
    "# 이 동적 특성 덕분에 디버깅이 쉽고, 유연한 모델 설계가 가능합니다. 모델의 구조를 쉽게 변경하거나 실시간으로 결과를 확인하면서 작업할 수 있습니다.\n",
    "# NumPy와 유사한 텐서(Tensor) 연산:\n",
    "\n",
    "# PyTorch는 Tensor라는 객체를 통해 다차원 배열을 처리합니다. Tensor는 NumPy 배열과 유사하지만, GPU를 통해 병렬 연산을 수행할 수 있다는 차이가 있습니다.\n",
    "# Tensor는 딥러닝 모델의 가중치, 입력, 출력 등으로 사용되며, 이로 인해 PyTorch는 GPU를 활용한 고속 연산을 지원합니다.\n",
    "# GPU 가속 지원:\n",
    "\n",
    "# PyTorch는 GPU를 사용하여 연산 속도를 크게 높일 수 있습니다. 간단하게 tensor.to('cuda') 명령을 통해 GPU에서 텐서 연산을 수행할 수 있습니다.\n",
    "# GPU 지원 덕분에 대용량 데이터셋이나 복잡한 신경망 모델을 학습할 때 훨씬 더 빠르게 처리할 수 있습니다.\n",
    "# 모듈형 설계:\n",
    "\n",
    "# PyTorch는 매우 직관적이고 모듈형으로 설계되어 있어, 복잡한 신경망 구조도 쉽게 정의할 수 있습니다. 모델을 구성하는 레이어, 손실 함수, 옵티마이저 등을 모두 모듈 단위로 구성할 수 있습니다.\n",
    "# 또한, PyTorch의 torch.nn.Module 클래스를 상속받아 커스텀 신경망을 직접 설계할 수 있습니다.\n",
    "# 풍부한 라이브러리와 생태계:\n",
    "\n",
    "# PyTorch는 연구와 상용 개발 모두에 적합한 다양한 도구들을 제공합니다. torchvision, torchaudio, torchtext 등은 컴퓨터 비전, 오디오 처리, 자연어 처리에 사용되는 데이터셋과 모델을 손쉽게 다룰 수 있는 라이브러리입니다.\n",
    "# 또한, PyTorch Lightning, fast.ai와 같은 고수준 API도 있어 빠르게 프로토타입을 만들고 학습을 진행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c514a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed656a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.20.0-cp39-cp39-win_amd64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Collecting torch==2.5.0 (from torchvision)\n",
      "  Downloading torch-2.5.0-cp39-cp39-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Collecting filelock (from torch==2.5.0->torchvision)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.5.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.5.0->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.5.0->torchvision) (3.0.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.5.0->torchvision) (2021.8.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Error parsing dependencies of ipykernel: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    matplotlib-inline (<0.2.0appnope,>=0.1.0) ; platform_system == \"Darwin\"\n",
      "                      ~~~~~~~~^\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\JYB\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\~orch'.\n",
      "  You can safely remove it manually.\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sympy==1.13.1 (from torch==2.5.0->torchvision)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy==1.13.1->torch==2.5.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch==2.5.0->torchvision) (2.0.1)\n",
      "Downloading torchvision-0.20.0-cp39-cp39-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 1.6/1.6 MB 112.1 kB/s eta 0:00:00\n",
      "Downloading torch-2.5.0-cp39-cp39-win_amd64.whl (203.0 MB)\n",
      "   ---------------------------------------- 203.0/203.0 MB 5.9 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 6.2/6.2 MB 5.7 MB/s eta 0:00:00\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: sympy, filelock, torch, torchvision\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.10.2\n",
      "    Uninstalling torch-1.10.2:\n",
      "      Successfully uninstalled torch-1.10.2\n",
      "Successfully installed filelock-3.16.1 sympy-1.13.1 torch-2.5.0 torchvision-0.20.0\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9540d3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해결 방법\n",
    "# 1. PyTorch 및 torchvision 최신 버전 재설치\n",
    "# 해당 오류는 PyTorch 및 torchvision의 버전 불일치나 손상된 설치로 인해 발생할 수 있습니다. \n",
    "# 먼저 PyTorch 및 torchvision을 최신 버전으로 재설치하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df580c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 및 관련 라이브러리 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eb67b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe039a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 및 torchvision 재설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b355db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d585ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afbf84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4a8ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736c622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 데이터 준비\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "model = MLP()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(5):\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(\"Training complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58785016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리:\n",
    "# transforms.ToTensor()는 이미지를 PyTorch 텐서로 변환합니다.\n",
    "# transforms.Normalize((0.5,), (0.5,))는 각 픽셀 값을 -1에서 1 사이로 정규화합니다.\n",
    "# 데이터셋 로드:\n",
    "# datasets.MNIST()를 통해 MNIST 데이터셋을 다운로드하고 로드합니다. train=True는 훈련 데이터를, train=False는 테스트 데이터를 가져옵니다.\n",
    "# DataLoader:\n",
    "# DataLoader는 데이터를 미니 배치로 나누고, 모델 학습 시 사용할 수 있도록 제공합니다. batch_size=64는 훈련 시 64개의 샘플을 한 번에 처리하고, 테스트 시에는 1000개의 샘플을 처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46710e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7488aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.MLP(다층 퍼셉트론) 모델:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a1044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 입력층: MNIST 데이터셋의 각 이미지는 28x28 픽셀 크기이므로, 이를 28 * 28로 펼쳐서 784개의 입력 노드를 가진 벡터로 변환합니다.\n",
    "# 은닉층: 은닉층은 128개의 노드로 구성되며, 활성화 함수로 ReLU(Rectified Linear Unit)를 사용합니다.\n",
    "# 출력층: 10개의 출력 노드로 구성되어 있으며, 이는 0부터 9까지의 숫자에 해당하는 클래스를 나타냅니다. log_softmax 함수를 사용하여 출력값을 확률 분포로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4b0038",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return torch.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9baf53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d808728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.손실 함수와 최적화 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc306462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수:\n",
    "# nn.NLLLoss()는 음의 로그 가능도 손실 함수로, log_softmax와 함께 사용됩니다. 이 함수는 분류 문제에서 자주 사용됩니다.\n",
    "# 최적화 기법:\n",
    "# Adam 옵티마이저를 사용하여 모델의 파라미터를 업데이트합니다. 학습률은 lr=0.001로 설정됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f33c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2052a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 학습 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efe6c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수:\n",
    "# model.train()은 모델을 학습 모드로 설정합니다.\n",
    "# 각 배치에 대해, optimizer.zero_grad()로 이전 배치의 그라디언트를 초기화하고, 모델의 예측값(output)과 실제값(target) 간의 손실을 계산합니다.\n",
    "# 그라디언트(loss.backward())를 통해 모델의 파라미터를 업데이트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a005c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd79afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 모델 평가 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0769e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 함수:\n",
    "# model.eval()은 평가 모드로 설정하며, torch.no_grad()는 평가 중에 그라디언트를 계산하지 않도록 설정하여 메모리와 계산 시간을 절약합니다.\n",
    "# 예측된 값(pred)과 실제 값(target)을 비교하여 정확도를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f18310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    print(f'Test Accuracy: {correct / len(test_loader.dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6aaa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec93a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 6):\n",
    "    train(model, train_loader, optimizer, criterion)\n",
    "    test(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115835a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
