{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a49269d",
   "metadata": {},
   "source": [
    "## Skip-gram모델을 활용 Word2Vec실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadea21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어(Stopword)를 제거하기 위해 nltk 패키지를 활용\n",
    "# https://direction-f.tistory.com/29  자료 겅리한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1200bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JYB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68674a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text8 데이터를 활용했으며, 이는 Kaggle 데이터셋에서 다운받을 수 있습니다. \n",
    "# 데이터를 읽어서 각 단어의 빈도를 계산하겠습니다. \n",
    "# 여기서는 읽은 데이터 중에서 10000개까지만 읽어서 진행하겠습니다.\n",
    "\n",
    "# nltk패키지를 이용하여 불용어를 제거하고, 단어가 나올때마다 +1를 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e96bfb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m13\u001b[0m\n\u001b[1;33m    wc_dict\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "wc_dict={}\n",
    "step=0      \n",
    "with open('./text8',\"r\") as f:\n",
    "    text = f.read()\n",
    "    sent = text[:10000].split()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    sent_filter = [w for w in sent if not w in stop_words]\n",
    "    for word in sent_filter:\n",
    "        step+=1\n",
    "        print(step)\n",
    "        wc_dict[word] = wc_dict.get(word,0)+1\n",
    "        \n",
    " wc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc851af",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = [\"<UNK>\"]+sorted(wc_dict, key =wc_dict.get, reverse= True)[:max_vocab-1] ## 큰수부터 max_vocab 까지 가지고 옮(여기선 전부)\n",
    "word2idx = {idx2word[idx]:idx for idx, _ in enumerate(idx2word)}\n",
    "vocab = set([word for word in word2idx]) ## dict의 ket값만 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cc1e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context데이터를 만들기 위한 함수를 정의하는데, \n",
    "# 첫 번째 단어같은 경우는 전에 나온 단어가 없기 때문에 \"<UNK>\"로 Padding을 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfc170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram(sentence, i, window_size = 2):\n",
    "    iword = sentence[i]\n",
    "    left = sentence[max(i-window_size,0):i] ## i(anchor)보다 전에 나온 단어들\n",
    "    right = sentence[i+1:i+1+window_size] ## i(anchor)보다 후에 나온 단어들\n",
    "    \n",
    "    \n",
    "    ## Padding 추가\n",
    "    return iword, [\"<UNK>\"for _ in range(window_size - len(left))] + left + right + [\"<UNK>\" for _ in range(window_size - len(right))]\n",
    "\n",
    "sent =[]\n",
    "data=[]\n",
    "step=0\n",
    "with open('./text8',\"r\") as f:\n",
    "    text = f.read()\n",
    "    words = text[:10000].split()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words_filter = [w for w in words if not w in stop_words]\n",
    "  \n",
    "    for word in words_filter:\n",
    "        \n",
    "        step +=1\n",
    "        print(step)\n",
    "        if word in vocab:            \n",
    "            sent.append(word)\n",
    "        \n",
    "        else :\n",
    "            sent.append(\"<UNK>\")\n",
    "        \n",
    "       \n",
    "    for i in range(len(sent)):\n",
    "        print(i)\n",
    "        iword, owords = skipgram(sent, i)\n",
    "        data.append((word2idx[iword], [word2idx[oword] for oword in owords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e9c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 코드를 실행하게 되면(1, [2,4,5,7])과 같이 구성되게 됩니다. \n",
    "# 즉 1번 단어에 앞뒤로 2,4,5,7번의 단어가 나오게 된 것입니다. 이번에 학습할때 우리는 pair 데이터를 활용하려고 합니다. \n",
    "# 따라서 (1, [2,4,5,7])를 (1,2), (1,4),(1,5),(1,7)과 같이 pair를 만들어 학습을 진행하고자 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc55b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, batch_size=950): ## batch_size는 조정가능\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(data)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = data[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii][0]\n",
    "            batch_y = batch[ii][1]\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y)) ## 같은 값을 (window_size)만큼 늘림\n",
    "        yield x, y \n",
    "        \n",
    "x,y=next(get_batches(data)) ## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac2bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주요 Concept은 관심단어를 Embedding 하고 \n",
    "# Context 데이터도 Embedding 하여 각 Embedding한 Vector들의 dot product값을 최대화하는 것과\n",
    "# 동시에 Negative Sampling한 데이터를 Embedding Vector와의 dot product는 최소화 하는 것입니다.\n",
    "# 위 그램에서 전자가 Context data와의 dot product를 나타내고 후자가 negative sampling한 데이터와의 dot product를 나타냅니다. 각 dot product한 값들에 Sigmoid function과 log를 취합니다. 위 함수를 최대화하도록 학습하게 됩니다. \n",
    "# 실제 Pytorch에서 표현할 때는 \"-\" 를 곱해주어 최소화 문제로 변경합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c3a1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramNeg(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed, noise_dist=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_embed = n_embed\n",
    "        self.noise_dist = noise_dist\n",
    "        \n",
    "        # define embedding layers for input and output words\n",
    "        self.in_embed = nn.Embedding(n_vocab,n_embed)\n",
    "        self.out_embed = nn.Embedding(n_vocab,n_embed)\n",
    "        \n",
    "        # Initialize both embedding tables with uniform distribution\n",
    "        self.in_embed.weight.data.uniform_(-1,1)\n",
    "        self.out_embed.weight.data.uniform_(-1,1)\n",
    "        \n",
    "    def forward_input(self, input_words):\n",
    "        \n",
    "        input_vector = self.in_embed(input_words)\n",
    "        return input_vector\n",
    "    \n",
    "    def forward_output(self, output_words):\n",
    "        \n",
    "        output_vector = self.out_embed(output_words)\n",
    "\n",
    "        return output_vector\n",
    "    \n",
    "    def forward_noise(self, batch_size, n_samples):\n",
    "        \"\"\" Generate noise vectors with shape (batch_size, n_samples, n_embed)\"\"\"\n",
    "        if self.noise_dist is None:\n",
    "            # Sample words uniformly\n",
    "            noise_dist = torch.ones(self.n_vocab)\n",
    "        else:\n",
    "            noise_dist = self.noise_dist\n",
    "            \n",
    "        # Sample words from our noise distribution\n",
    "        noise_words = torch.multinomial(noise_dist,\n",
    "                                        batch_size * n_samples,\n",
    "                                        replacement=True)  ## noise sample 만큼 데이터 생성 \n",
    "    \n",
    "\n",
    "        noise_vector = self.out_embed(noise_words).view(batch_size,n_samples,self.n_embed)        \n",
    "        return noise_vector\n",
    "        \n",
    "class NegativeSamplingLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_vectors, output_vectors, noise_vectors):\n",
    "        \n",
    "        batch_size, embed_size = input_vectors.shape\n",
    "        \n",
    "        # Input vectors should be a batch of column vectors\n",
    "        input_vectors = input_vectors.view(batch_size, embed_size, 1)\n",
    "        \n",
    "        # Output vectors should be a batch of row vectors\n",
    "        output_vectors = output_vectors.view(batch_size, 1, embed_size)\n",
    "        \n",
    "        # bmm = batch matrix multiplication (b*n*e)(b*e*n) = (b*n*n)\n",
    "        # correct log-sigmoid loss\n",
    "        out_loss = torch.bmm(output_vectors, input_vectors).sigmoid().log()\n",
    "        out_loss = out_loss.squeeze()\n",
    "        \n",
    "        \n",
    "        # incorrect log-sigmoid loss\n",
    "        noise_loss = torch.bmm(noise_vectors.neg(), input_vectors).sigmoid().log()\n",
    "        ## 각 row별로 loss 합산(e.g. [input-negative1 loss] +[input-negative2 loss]\n",
    "        noise_loss = noise_loss.squeeze().sum(1)  \n",
    "        return -(out_loss + noise_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ebdbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Sampling을 하기 위한 Noise Distribution을 생성해줍니다. \n",
    "# torch.mutinomial에서 input으로 들어가는 값은 각 index를 반환할 확률이라고 이해하시면 될 것 같습니다. \n",
    "# 따라서 Noise Distribution을 생성한다는 것은 어떤 index를 반환할지에 대한 확률값을 주는 것입니다. \n",
    "\n",
    "# 확률값을 줄 때, 우리 단어중에서 나온 빈도가 높은 단어가 높은 확률을 가지는 것은 자연스러울 것입니다. \n",
    "# 따라서 빈도가 많이 나온 단어들은 상대적으로 높은 확률값을 가지게 됩니다. \n",
    "# 우리 데이터 원본에는 \"<UNK>\"가 없기 때문에 별도로 index 0자리에 생성확률 0으로 추가해주었습니다. \n",
    "# negative sample로 뽑힐 확률은 아래와 같이 3/4승을 많이 적용합니다.(실험적으로 성능이 좋다고 합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6efb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = len(idx2word)\n",
    "freqs = {word:count/total_count for word, count in wc_dict.items()} \n",
    "word_freqs = np.array(sorted(freqs.values(), reverse=True))\n",
    "word_freqs = np.concatenate(([0],word_freqs)) ## \"<UNK>\" 추가\n",
    "unigram_dist = word_freqs/word_freqs.sum()\n",
    "noise_dist = torch.from_numpy(unigram_dist**(0.75)/np.sum(unigram_dist**(0.75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa84ca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 30\n",
    "model = SkipGramNeg(len(vocab), embedding_dim, noise_dist =noise_dist)\n",
    "\n",
    "criterion = NegativeSamplingLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr =0.03)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for input_words, target_words in get_batches(data):\n",
    "        step +=1\n",
    "        inputs, targets = torch.LongTensor(input_words), torch.LongTensor(target_words)\n",
    "        input_vectors = model.forward_input(inputs)\n",
    "        output_vectors = model.forward_output(targets)\n",
    "        noise_vectors = model.forward_noise(inputs.shape[0], 5)\n",
    "        \n",
    "        loss = criterion(input_vectors, output_vectors, noise_vectors)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"Epoch: {}/{}\".format(epoch+1, 1000))\n",
    "            print(\"Loss: \", loss.item())​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24b3b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding된 벡터를 활용하여, Cosine similarity를 이용해 가장 가까이 있는 단어 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3711feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "embeddings = model.in_embed.weight.data.numpy()\n",
    "def cosine_(data):\n",
    "    close_idx= np.zeros([len(data),len(data)])\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data)):\n",
    "            close_idx[i,j]=cosine_similarity (data[i].reshape(1,-1),data[j].reshape(1,-1))\n",
    "            \n",
    "    return close_idx\n",
    "\n",
    "close_idx=cosine_(test_results)\n",
    "\n",
    "def closest_word(close_idx,word, word2idx, idx2word):\n",
    "    idx=word2idx[word]\n",
    "    close_idx_=close_idx[idx].argsort()[:5]\n",
    "    close_Word = [idx2word[i] for i in close_idx_]\n",
    "    return close_Word\n",
    "\n",
    "for i in vocab:    \n",
    "    sim_list = closest_word(close_idx, i, word2idx, idx2word)\n",
    "    \n",
    "    if \"<UNK>\" in sim_list:\n",
    "        sim_list.remove(\"<UNK>\")\n",
    "    print(\"word:{} : {}\".format(i,sim_list))\n",
    "    \n",
    "## Output\n",
    "##word:twenty : ['spirit', 'holy', 'citium', 'premise']\n",
    "##word:owners : ['authoritarian', 'communists', 'movements', 'polarised', 'later']\n",
    "##word:doctrine : ['access', 'communism', 'supported', 'way', 'favoured']\n",
    "##word:mikhail : ['word', 'institutions', 'anarchy', 'private']\n",
    "##word:new : ['incorporated', 'used', 'history', 'harsh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b6365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE()를 활용해서 시각화를 해보겠습니다. \n",
    "# TSNE는 고차원의 벡터를 저차원의 벡터로 차원 축소해주는 알고리즘입니다.\n",
    "# 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08834c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "viz_words = len(idx2word)\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embeddings[:viz_words, :])\n",
    "embed_tsne_plot =pd.DataFrame(embed_tsne, columns =[\"x\",\"y\"])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "plt.scatter(embed_tsne_plot[\"x\"],embed_tsne_plot[\"y\"], color='steelblue' )\n",
    "for idx in range(viz_words):\n",
    "    plt.annotate(idx2word[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d9fc43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
