{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c66914b-8d46-4897-b71e-658abbea0a38",
   "metadata": {},
   "source": [
    "## 1. Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c664ab-9f70-4010-b7ef-b88ede0d7a10",
   "metadata": {},
   "source": [
    "Deep Q-Network (DQN)**은 딥 강화 학습(Deep Reinforcement Learning)에서 자주 사용되는 알고리즘으로, Q-러닝과 신경망을 결합하여 복잡한 환경에서 학습할 수 있도록 합니다. DQN은 특히 게임 AI나 자동화된 트레이딩 시스템에서 널리 사용됩니다.\n",
    "\n",
    "Deep Q-Network (DQN) 개요\n",
    "    \n",
    "목표: 에이전트가 환경과 상호작용하면서, 최적의 행동을 학습해 누적 보상을 최대화하는 것입니다.\n",
    "\n",
    "Q-러닝: 𝑄(𝑠,𝑎)는 특정 상태 \n",
    "    \n",
    "𝑠에서 행동 𝑎를 취했을 때 예상되는 미래 보상입니다.\n",
    "    \n",
    "DQN 구조: 신경망을 사용하여 𝑄 함수를 근사화하며, 이를 통해 상태-행동 가치 함수를 학습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b5344c-a319-47df-86e7-ca406f5d3eac",
   "metadata": {},
   "source": [
    "### 1) 자동화된 트레이딩 시스템 사례 (삼성전자)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048c4fed-b024-480b-8d12-de4aee51b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 설명\n",
    "# 삼성전자 주식 데이터:\n",
    "\n",
    "# yfinance 라이브러리를 사용해 삼성전자의 주식 데이터를 다운로드하고, 수익률(Return)을 계산합니다.\n",
    "# StockTradingEnv 클래스:\n",
    "\n",
    "# OpenAI Gym 스타일의 환경을 정의하여, 에이전트가 주식을 매수, 매도, 보유할 수 있도록 합니다.\n",
    "# DQN 에이전트:\n",
    "\n",
    "# 신경망을 사용해 Q-함수를 근사화하고, 탐험-탐사 균형을 조절합니다 (epsilon-greedy 전략).\n",
    "# 학습 및 평가:\n",
    "\n",
    "# 에이전트는 여러 에피소드를 통해 학습하고, 점점 더 높은 보상을 목표로 행동을 최적화합니다.\n",
    "# 결과 시각화:\n",
    "\n",
    "# 삼성전자 주식의 종가 그래프를 시각화합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a834d-948c-494d-a2f0-ffe662e7d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# FutureWarning 무시\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# 1. 삼성전자 주식 데이터 로드\n",
    "df = yf.download(\"005930.KS\", start=\"2020-01-01\", end=\"2023-01-01\")\n",
    "\n",
    "# 인덱스의 날짜를 UTC로 변환하여 tz_localize 에러 해결\n",
    "df.index = pd.to_datetime(df.index, utc=True)\n",
    "\n",
    "# 결측값 처리\n",
    "df[\"Dividends\"] = df[\"Dividends\"].fillna(0)\n",
    "df[\"Stock Splits\"] = df[\"Stock Splits\"].fillna(0)\n",
    "df['Return'] = df['Close'].pct_change()\n",
    "df = df.dropna()\n",
    "print(df.head())\n",
    "\n",
    "# 2. 트레이딩 환경 정의\n",
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.n_step = len(data)\n",
    "        self.current_step = 0\n",
    "        self.balance = 1000000  # 초기 자본 (1,000,000 원)\n",
    "        self.shares = 0  # 보유 주식 수\n",
    "        self.total_profit = 0\n",
    "        self.action_space = spaces.Discrete(3)  # [0: 보유, 1: 매수, 2: 매도]\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(5,), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.balance = 1000000\n",
    "        self.shares = 0\n",
    "        self.total_profit = 0\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = np.array([\n",
    "            self.data.iloc[self.current_step]['Close'],\n",
    "            self.data.iloc[self.current_step]['Return'],\n",
    "            self.balance,\n",
    "            self.shares,\n",
    "            self.total_profit\n",
    "        ])\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        current_price = self.data.iloc[self.current_step]['Close']\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        # 행동: 0(보유), 1(매수), 2(매도)\n",
    "        if action == 1:  # 매수\n",
    "            if self.balance >= current_price:\n",
    "                self.shares += 1\n",
    "                self.balance -= current_price\n",
    "        elif action == 2:  # 매도\n",
    "            if self.shares > 0:\n",
    "                self.shares -= 1\n",
    "                self.balance += current_price\n",
    "                reward = current_price  # 이익을 보상으로\n",
    "\n",
    "        self.total_profit = self.balance + self.shares * current_price - 1000000\n",
    "        self.current_step += 1\n",
    "\n",
    "        if self.current_step >= self.n_step - 1:\n",
    "            done = True\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "# 3. DQN 모델 정의\n",
    "def build_model(state_size, action_size):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "# 4. DQN 에이전트 정의\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = []\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = build_model(state_size, action_size)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# 5. 환경 설정 및 학습\n",
    "env = StockTradingEnv(df)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "episodes = 100\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    agent.replay(32)\n",
    "    print(f\"Episode {e + 1}/{episodes}, Total Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.2f}\")\n",
    "\n",
    "# 6. 결과 시각화\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df.index, df['Close'], label='Samsung Electronics Close Price')\n",
    "plt.title('Samsung Electronics Stock Trading with DQN')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673fad60-7992-45f1-be2a-46e0c477496f",
   "metadata": {},
   "source": [
    "결과 분석\n",
    "학습이 진행됨에 따라, 에이전트는 점점 더 나은 매수/매도 전략을 학습하게 됩니다.\n",
    "초기에는 무작위 탐험(epsilon=1.0)이 많지만, 시간이 지남에 따라 탐험이 줄고 탐사가 늘어납니다 (epsilon 감소)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e179720f-34aa-431a-9a70-b774e2852e54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e2ed1b5-df9d-4407-a362-cc86cfd5c407",
   "metadata": {},
   "source": [
    "### 2) KOSPI 데이터와 DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ee53be-1bf6-4115-ad47-a6fa4d312174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc01f77-01da-4b01-802a-ffc7928a6716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# FutureWarning 무시\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# 1. KOSPI 데이터 로드\n",
    "df = yf.download(\"^KS11\", start=\"2020-01-01\", end=\"2023-01-01\")\n",
    "\n",
    "# 인덱스의 날짜를 UTC로 변환하여 tz_localize 에러 해결\n",
    "df.index = pd.to_datetime(df.index, utc=True)\n",
    "\n",
    "# 결측값 처리\n",
    "df[\"Dividends\"] = df[\"Dividends\"].fillna(0)\n",
    "df[\"Stock Splits\"] = df[\"Stock Splits\"].fillna(0)\n",
    "df['Return'] = df['Close'].pct_change()\n",
    "df = df.dropna()\n",
    "print(df.head())\n",
    "\n",
    "# 2. 트레이딩 환경 정의\n",
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.n_step = len(data)\n",
    "        self.current_step = 0\n",
    "        self.balance = 1000000  # 초기 자본 (1,000,000 원)\n",
    "        self.shares = 0  # 보유 주식 수\n",
    "        self.total_profit = 0\n",
    "        self.action_space = spaces.Discrete(3)  # [0: 보유, 1: 매수, 2: 매도]\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(5,), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.balance = 1000000\n",
    "        self.shares = 0\n",
    "        self.total_profit = 0\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = np.array([\n",
    "            self.data.iloc[self.current_step]['Close'],\n",
    "            self.data.iloc[self.current_step]['Return'],\n",
    "            self.balance,\n",
    "            self.shares,\n",
    "            self.total_profit\n",
    "        ])\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        current_price = self.data.iloc[self.current_step]['Close']\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        # 행동: 0(보유), 1(매수), 2(매도)\n",
    "        if action == 1:  # 매수\n",
    "            if self.balance >= current_price:\n",
    "                self.shares += 1\n",
    "                self.balance -= current_price\n",
    "        elif action == 2:  # 매도\n",
    "            if self.shares > 0:\n",
    "                self.shares -= 1\n",
    "                self.balance += current_price\n",
    "                reward = current_price  # 이익을 보상으로\n",
    "\n",
    "        self.total_profit = self.balance + self.shares * current_price - 1000000\n",
    "        self.current_step += 1\n",
    "\n",
    "        if self.current_step >= self.n_step - 1:\n",
    "            done = True\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "# 3. DQN 모델 정의\n",
    "def build_model(state_size, action_size):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "# 4. DQN 에이전트 정의\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = []\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = build_model(state_size, action_size)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# 5. 환경 설정 및 학습\n",
    "env = StockTradingEnv(df)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "episodes = 100\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    agent.replay(32)\n",
    "    print(f\"Episode {e + 1}/{episodes}, Total Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.2f}\")\n",
    "\n",
    "# 6. 결과 시각화\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df.index, df['Close'], label='KOSPI Close Price')\n",
    "plt.title('KOSPI Index Trading with DQN')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b369312-ffe8-4caf-98c3-4e82d64af005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
