{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c66914b-8d46-4897-b71e-658abbea0a38",
   "metadata": {},
   "source": [
    "## 1. Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c664ab-9f70-4010-b7ef-b88ede0d7a10",
   "metadata": {},
   "source": [
    "Deep Q-Network (DQN)**ÏùÄ Îî• Í∞ïÌôî ÌïôÏäµ(Deep Reinforcement Learning)ÏóêÏÑú ÏûêÏ£º ÏÇ¨Ïö©ÎêòÎäî ÏïåÍ≥†Î¶¨Ï¶òÏúºÎ°ú, Q-Îü¨ÎãùÍ≥º Ïã†Í≤ΩÎßùÏùÑ Í≤∞Ìï©ÌïòÏó¨ Î≥µÏû°Ìïú ÌôòÍ≤ΩÏóêÏÑú ÌïôÏäµÌï† Ïàò ÏûàÎèÑÎ°ù Ìï©ÎãàÎã§. DQNÏùÄ ÌäπÌûà Í≤åÏûÑ AIÎÇò ÏûêÎèôÌôîÎêú Ìä∏Î†àÏù¥Îî© ÏãúÏä§ÌÖúÏóêÏÑú ÎÑêÎ¶¨ ÏÇ¨Ïö©Îê©ÎãàÎã§.\n",
    "\n",
    "Deep Q-Network (DQN) Í∞úÏöî\n",
    "    \n",
    "Î™©Ìëú: ÏóêÏù¥Ï†ÑÌä∏Í∞Ä ÌôòÍ≤ΩÍ≥º ÏÉÅÌò∏ÏûëÏö©ÌïòÎ©¥ÏÑú, ÏµúÏ†ÅÏùò ÌñâÎèôÏùÑ ÌïôÏäµÌï¥ ÎàÑÏ†Å Î≥¥ÏÉÅÏùÑ ÏµúÎåÄÌôîÌïòÎäî Í≤ÉÏûÖÎãàÎã§.\n",
    "\n",
    "Q-Îü¨Îãù: ùëÑ(ùë†,ùëé)Îäî ÌäπÏ†ï ÏÉÅÌÉú \n",
    "    \n",
    "ùë†ÏóêÏÑú ÌñâÎèô ùëéÎ•º Ï∑®ÌñàÏùÑ Îïå ÏòàÏÉÅÎêòÎäî ÎØ∏Îûò Î≥¥ÏÉÅÏûÖÎãàÎã§.\n",
    "    \n",
    "DQN Íµ¨Ï°∞: Ïã†Í≤ΩÎßùÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ùëÑ Ìï®ÏàòÎ•º Í∑ºÏÇ¨ÌôîÌïòÎ©∞, Ïù¥Î•º ÌÜµÌï¥ ÏÉÅÌÉú-ÌñâÎèô Í∞ÄÏπò Ìï®ÏàòÎ•º ÌïôÏäµÌï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b5344c-a319-47df-86e7-ca406f5d3eac",
   "metadata": {},
   "source": [
    "### 1) ÏûêÎèôÌôîÎêú Ìä∏Î†àÏù¥Îî© ÏãúÏä§ÌÖú ÏÇ¨Î°Ä (ÏÇºÏÑ±Ï†ÑÏûê)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048c4fed-b024-480b-8d12-de4aee51b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏΩîÎìú ÏÑ§Î™Ö\n",
    "# ÏÇºÏÑ±Ï†ÑÏûê Ï£ºÏãù Îç∞Ïù¥ÌÑ∞:\n",
    "\n",
    "# yfinance ÎùºÏù¥Î∏åÎü¨Î¶¨Î•º ÏÇ¨Ïö©Ìï¥ ÏÇºÏÑ±Ï†ÑÏûêÏùò Ï£ºÏãù Îç∞Ïù¥ÌÑ∞Î•º Îã§Ïö¥Î°úÎìúÌïòÍ≥†, ÏàòÏùµÎ•†(Return)ÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§.\n",
    "# StockTradingEnv ÌÅ¥ÎûòÏä§:\n",
    "\n",
    "# OpenAI Gym Ïä§ÌÉÄÏùºÏùò ÌôòÍ≤ΩÏùÑ Ï†ïÏùòÌïòÏó¨, ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Ï£ºÏãùÏùÑ Îß§Ïàò, Îß§ÎèÑ, Î≥¥Ïú†Ìï† Ïàò ÏûàÎèÑÎ°ù Ìï©ÎãàÎã§.\n",
    "# DQN ÏóêÏù¥Ï†ÑÌä∏:\n",
    "\n",
    "# Ïã†Í≤ΩÎßùÏùÑ ÏÇ¨Ïö©Ìï¥ Q-Ìï®ÏàòÎ•º Í∑ºÏÇ¨ÌôîÌïòÍ≥†, ÌÉêÌóò-ÌÉêÏÇ¨ Í∑†ÌòïÏùÑ Ï°∞Ï†àÌï©ÎãàÎã§ (epsilon-greedy Ï†ÑÎûµ).\n",
    "# ÌïôÏäµ Î∞è ÌèâÍ∞Ä:\n",
    "\n",
    "# ÏóêÏù¥Ï†ÑÌä∏Îäî Ïó¨Îü¨ ÏóêÌîºÏÜåÎìúÎ•º ÌÜµÌï¥ ÌïôÏäµÌïòÍ≥†, Ï†êÏ†ê Îçî ÎÜíÏùÄ Î≥¥ÏÉÅÏùÑ Î™©ÌëúÎ°ú ÌñâÎèôÏùÑ ÏµúÏ†ÅÌôîÌï©ÎãàÎã§.\n",
    "# Í≤∞Í≥º ÏãúÍ∞ÅÌôî:\n",
    "\n",
    "# ÏÇºÏÑ±Ï†ÑÏûê Ï£ºÏãùÏùò Ï¢ÖÍ∞Ä Í∑∏ÎûòÌîÑÎ•º ÏãúÍ∞ÅÌôîÌï©ÎãàÎã§.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a834d-948c-494d-a2f0-ffe662e7d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# FutureWarning Î¨¥Ïãú\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# 1. ÏÇºÏÑ±Ï†ÑÏûê Ï£ºÏãù Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "df = yf.download(\"005930.KS\", start=\"2020-01-01\", end=\"2023-01-01\")\n",
    "\n",
    "# Ïù∏Îç±Ïä§Ïùò ÎÇ†ÏßúÎ•º UTCÎ°ú Î≥ÄÌôòÌïòÏó¨ tz_localize ÏóêÎü¨ Ìï¥Í≤∞\n",
    "df.index = pd.to_datetime(df.index, utc=True)\n",
    "\n",
    "# Í≤∞Ï∏°Í∞í Ï≤òÎ¶¨\n",
    "df[\"Dividends\"] = df[\"Dividends\"].fillna(0)\n",
    "df[\"Stock Splits\"] = df[\"Stock Splits\"].fillna(0)\n",
    "df['Return'] = df['Close'].pct_change()\n",
    "df = df.dropna()\n",
    "print(df.head())\n",
    "\n",
    "# 2. Ìä∏Î†àÏù¥Îî© ÌôòÍ≤Ω Ï†ïÏùò\n",
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.n_step = len(data)\n",
    "        self.current_step = 0\n",
    "        self.balance = 1000000  # Ï¥àÍ∏∞ ÏûêÎ≥∏ (1,000,000 Ïõê)\n",
    "        self.shares = 0  # Î≥¥Ïú† Ï£ºÏãù Ïàò\n",
    "        self.total_profit = 0\n",
    "        self.action_space = spaces.Discrete(3)  # [0: Î≥¥Ïú†, 1: Îß§Ïàò, 2: Îß§ÎèÑ]\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(5,), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.balance = 1000000\n",
    "        self.shares = 0\n",
    "        self.total_profit = 0\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = np.array([\n",
    "            self.data.iloc[self.current_step]['Close'],\n",
    "            self.data.iloc[self.current_step]['Return'],\n",
    "            self.balance,\n",
    "            self.shares,\n",
    "            self.total_profit\n",
    "        ])\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        current_price = self.data.iloc[self.current_step]['Close']\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        # ÌñâÎèô: 0(Î≥¥Ïú†), 1(Îß§Ïàò), 2(Îß§ÎèÑ)\n",
    "        if action == 1:  # Îß§Ïàò\n",
    "            if self.balance >= current_price:\n",
    "                self.shares += 1\n",
    "                self.balance -= current_price\n",
    "        elif action == 2:  # Îß§ÎèÑ\n",
    "            if self.shares > 0:\n",
    "                self.shares -= 1\n",
    "                self.balance += current_price\n",
    "                reward = current_price  # Ïù¥ÏùµÏùÑ Î≥¥ÏÉÅÏúºÎ°ú\n",
    "\n",
    "        self.total_profit = self.balance + self.shares * current_price - 1000000\n",
    "        self.current_step += 1\n",
    "\n",
    "        if self.current_step >= self.n_step - 1:\n",
    "            done = True\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "# 3. DQN Î™®Îç∏ Ï†ïÏùò\n",
    "def build_model(state_size, action_size):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "# 4. DQN ÏóêÏù¥Ï†ÑÌä∏ Ï†ïÏùò\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = []\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = build_model(state_size, action_size)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# 5. ÌôòÍ≤Ω ÏÑ§Ï†ï Î∞è ÌïôÏäµ\n",
    "env = StockTradingEnv(df)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "episodes = 100\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    agent.replay(32)\n",
    "    print(f\"Episode {e + 1}/{episodes}, Total Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.2f}\")\n",
    "\n",
    "# 6. Í≤∞Í≥º ÏãúÍ∞ÅÌôî\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df.index, df['Close'], label='Samsung Electronics Close Price')\n",
    "plt.title('Samsung Electronics Stock Trading with DQN')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673fad60-7992-45f1-be2a-46e0c477496f",
   "metadata": {},
   "source": [
    "Í≤∞Í≥º Î∂ÑÏÑù\n",
    "ÌïôÏäµÏù¥ ÏßÑÌñâÎê®Ïóê Îî∞Îùº, ÏóêÏù¥Ï†ÑÌä∏Îäî Ï†êÏ†ê Îçî ÎÇòÏùÄ Îß§Ïàò/Îß§ÎèÑ Ï†ÑÎûµÏùÑ ÌïôÏäµÌïòÍ≤å Îê©ÎãàÎã§.\n",
    "Ï¥àÍ∏∞ÏóêÎäî Î¨¥ÏûëÏúÑ ÌÉêÌóò(epsilon=1.0)Ïù¥ ÎßéÏßÄÎßå, ÏãúÍ∞ÑÏù¥ ÏßÄÎÇ®Ïóê Îî∞Îùº ÌÉêÌóòÏù¥ Ï§ÑÍ≥† ÌÉêÏÇ¨Í∞Ä ÎäòÏñ¥ÎÇ©ÎãàÎã§ (epsilon Í∞êÏÜå)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e179720f-34aa-431a-9a70-b774e2852e54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e2ed1b5-df9d-4407-a362-cc86cfd5c407",
   "metadata": {},
   "source": [
    "### 2) KOSPI Îç∞Ïù¥ÌÑ∞ÏôÄ DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ee53be-1bf6-4115-ad47-a6fa4d312174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc01f77-01da-4b01-802a-ffc7928a6716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# FutureWarning Î¨¥Ïãú\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# 1. KOSPI Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "df = yf.download(\"^KS11\", start=\"2020-01-01\", end=\"2023-01-01\")\n",
    "\n",
    "# Ïù∏Îç±Ïä§Ïùò ÎÇ†ÏßúÎ•º UTCÎ°ú Î≥ÄÌôòÌïòÏó¨ tz_localize ÏóêÎü¨ Ìï¥Í≤∞\n",
    "df.index = pd.to_datetime(df.index, utc=True)\n",
    "\n",
    "# Í≤∞Ï∏°Í∞í Ï≤òÎ¶¨\n",
    "df[\"Dividends\"] = df[\"Dividends\"].fillna(0)\n",
    "df[\"Stock Splits\"] = df[\"Stock Splits\"].fillna(0)\n",
    "df['Return'] = df['Close'].pct_change()\n",
    "df = df.dropna()\n",
    "print(df.head())\n",
    "\n",
    "# 2. Ìä∏Î†àÏù¥Îî© ÌôòÍ≤Ω Ï†ïÏùò\n",
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.n_step = len(data)\n",
    "        self.current_step = 0\n",
    "        self.balance = 1000000  # Ï¥àÍ∏∞ ÏûêÎ≥∏ (1,000,000 Ïõê)\n",
    "        self.shares = 0  # Î≥¥Ïú† Ï£ºÏãù Ïàò\n",
    "        self.total_profit = 0\n",
    "        self.action_space = spaces.Discrete(3)  # [0: Î≥¥Ïú†, 1: Îß§Ïàò, 2: Îß§ÎèÑ]\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(5,), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.balance = 1000000\n",
    "        self.shares = 0\n",
    "        self.total_profit = 0\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = np.array([\n",
    "            self.data.iloc[self.current_step]['Close'],\n",
    "            self.data.iloc[self.current_step]['Return'],\n",
    "            self.balance,\n",
    "            self.shares,\n",
    "            self.total_profit\n",
    "        ])\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        current_price = self.data.iloc[self.current_step]['Close']\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        # ÌñâÎèô: 0(Î≥¥Ïú†), 1(Îß§Ïàò), 2(Îß§ÎèÑ)\n",
    "        if action == 1:  # Îß§Ïàò\n",
    "            if self.balance >= current_price:\n",
    "                self.shares += 1\n",
    "                self.balance -= current_price\n",
    "        elif action == 2:  # Îß§ÎèÑ\n",
    "            if self.shares > 0:\n",
    "                self.shares -= 1\n",
    "                self.balance += current_price\n",
    "                reward = current_price  # Ïù¥ÏùµÏùÑ Î≥¥ÏÉÅÏúºÎ°ú\n",
    "\n",
    "        self.total_profit = self.balance + self.shares * current_price - 1000000\n",
    "        self.current_step += 1\n",
    "\n",
    "        if self.current_step >= self.n_step - 1:\n",
    "            done = True\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "# 3. DQN Î™®Îç∏ Ï†ïÏùò\n",
    "def build_model(state_size, action_size):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "# 4. DQN ÏóêÏù¥Ï†ÑÌä∏ Ï†ïÏùò\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = []\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = build_model(state_size, action_size)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# 5. ÌôòÍ≤Ω ÏÑ§Ï†ï Î∞è ÌïôÏäµ\n",
    "env = StockTradingEnv(df)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "episodes = 100\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    agent.replay(32)\n",
    "    print(f\"Episode {e + 1}/{episodes}, Total Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.2f}\")\n",
    "\n",
    "# 6. Í≤∞Í≥º ÏãúÍ∞ÅÌôî\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df.index, df['Close'], label='KOSPI Close Price')\n",
    "plt.title('KOSPI Index Trading with DQN')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b369312-ffe8-4caf-98c3-4e82d64af005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
