{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7ff3ef3",
   "metadata": {
    "id": "a7ff3ef3"
   },
   "source": [
    "자연어 처리란?\n",
    "\n",
    "인간의 언어를 컴퓨터가 이해하고 처리할 수 있도록 하는 기술을 연구하는 분야입니다.\n",
    "\n",
    "NLP는 컴퓨터 과학, 인공지능, 언어학의 교차점에 위치하며, 텍스트 및 음성 데이터를 분석, 해석, 생성하는 다양한 응용 프로그램에서 사용됩니다.\n",
    "주요 목표는 컴퓨터가 자연어(사람들이 일상에서 사용하는 언어)를 이해하고 상호작용할 수 있도록 하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9d1617",
   "metadata": {
    "id": "0c9d1617"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b42050f",
   "metadata": {
    "id": "1b42050f"
   },
   "outputs": [],
   "source": [
    " # NLP에 필요한 spaCy, NLTK, Transformers 등의 라이브러리 동시에 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d20e14e",
   "metadata": {
    "id": "1d20e14e",
    "outputId": "5a94c610-1308-44b9-dd12-bcf0e397d6a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.7)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: spacy in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.45.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Error parsing dependencies of ipykernel: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    matplotlib-inline (<0.2.0appnope,>=0.1.0) ; platform_system == \"Darwin\"\n",
      "                      ~~~~~~~~^\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: click in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (2022.1.18)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.61.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (0.12.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (2.26.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (3.0.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (73.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.26.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.12.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jyb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "pip install nltk spacy transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11ac957",
   "metadata": {
    "id": "e11ac957"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced565e9",
   "metadata": {
    "id": "ced565e9"
   },
   "outputs": [],
   "source": [
    "#  패키지 관리 명령어 요약\n",
    "# 명령어\t설명\n",
    "# pip install 패키지명\t새로운 패키지를 설치합니다.\n",
    "# pip list\t현재 설치된 패키지 목록을 확인합니다.\n",
    "# pip install --upgrade 패키지명\t패키지를 최신 버전으로 업데이트합니다.\n",
    "# pip uninstall 패키지명\t패키지를 삭제합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07d899c",
   "metadata": {
    "id": "a07d899c"
   },
   "outputs": [],
   "source": [
    "# NLTK란?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777011c0",
   "metadata": {
    "id": "777011c0"
   },
   "outputs": [],
   "source": [
    "# NLTK(Natural Language Toolkit)는 파이썬을 기반으로 자연어 처리(NLP)를 수행할 수 있는 강력한 오픈소스 라이브러리입니다.\n",
    "# 자연어 처리의 입문자부터 전문가까지 다양한 사용자들이 손쉽게 사용할 수 있도록 다양한 도구와 데이터셋을 제공하며, 텍스트 전처리, 형태소 분석, 문장 분류, 정서 분석 등의 과업을 지원합니다. NLTK는 학습, 연구, 프로토타입 개발에 적합한 다목적 도구 상자로, 다양한 NLP 과업을 체계적으로 처리할 수 있는 모듈을 포함하고 있습니다.\n",
    "\n",
    "# NLTK의 주요 특징\n",
    "# 텍스트 전처리: NLTK는 토큰화, 불용어 제거, 어간 추출, 표제어 추출 등 텍스트를 분석하기 전에 필요한 전처리 과정을 쉽게 수행할 수 있는 도구들을 제공합니다.\n",
    "# 형태소 분석: 단어의 기본 형태를 파악하는 형태소 분석 기능을 제공하여 텍스트에서 단어의 형태나 품사를 쉽게 분석할 수 있습니다.\n",
    "# 정서 분석: 텍스트에 포함된 정서을 분석하는 데 필요한 도구도 제공하여, 긍정적인지 부정적인지 등의 정서 분석을 쉽게 수행할 수 있습니다.\n",
    "# 말뭉치(Corpus) 데이터 제공: NLTK는 여러 말뭉치(corpus) 데이터를 포함하고 있어, 학습이나 실습에서 바로 사용할 수 있습니다. 예를 들어, 브라운 코퍼스(Brown Corpus), 워드넷(WordNet) 등 다양한 텍스트 데이터가 내장되어 있어 학습 자료로 유용합니다.\n",
    "# 분류 및 모델링: NLTK는 기계 학습 기반 분류, 군집화 등의 텍스트 분류 작업을 지원하며, 이를 통해 자연어 처리 모델을 구축할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6974e658",
   "metadata": {
    "id": "6974e658"
   },
   "outputs": [],
   "source": [
    "# NLTK 주요 데이터셋 소개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9af9137",
   "metadata": {
    "id": "f9af9137"
   },
   "outputs": [],
   "source": [
    "# NLTK는 다양한 말뭉치(corpus)를 제공하며, 이를 이용해 자연어 처리 실습 및 연구를 수행할 수 있습니다.\n",
    "# 다음은 자주 사용하는 데이터셋입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd40156",
   "metadata": {
    "id": "6cd40156"
   },
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')  # 워드넷(WordNet): 어휘 데이터베이스\n",
    "# nltk.download('averaged_perceptron_tagger')  # 품사 태깅 데이터\n",
    "# nltk.download('stopwords')  # 불용어 목록\n",
    "# nltk.download('movie_reviews')  # 영화 리뷰 말뭉치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dac8ec",
   "metadata": {
    "id": "a5dac8ec"
   },
   "outputs": [],
   "source": [
    "# NLTK(Natural Language Toolkit)는 파이썬 기반의 자연어 처리 라이브러리로, 다양한 텍스트 처리 기능을 제공하며 풍부한 데이터셋을 포함하고 있습니다. 이러한 데이터셋은 자연어 처리 연구와 학습에 널리 사용됩니다. 아래는 NLTK에서 제공하는 주요 데이터셋들에 대한 간략한 소개입니다:\n",
    "\n",
    "# 1. Brown Corpus\n",
    "# 설명: Brown Corpus는 최초로 수집된 대규모 언어 코퍼스 중 하나로, 1960년대 미국 영어 텍스트로 구성되어 있습니다. 약 1백만 개의 단어로 구성된 이 코퍼스는 장르별로 문서를 나누어 제공하여 다양한 텍스트 장르(소설, 뉴스, 학술 등)에서의 언어 패턴을 연구할 수 있습니다.\n",
    "# 용도: 주로 언어 모델링, 품사 태깅 연구에 사용됩니다.\n",
    "# 2. Gutenberg Corpus\n",
    "# 설명: Project Gutenberg에서 제공하는 공공 도메인 텍스트로 구성된 코퍼스입니다. 이 코퍼스는 문학 작품들을 포함하고 있으며, 셰익스피어, 제인 오스틴, 마크 트웨인 등 유명 작가들의 작품이 포함되어 있습니다.\n",
    "# 용도: 자연어 처리 연구, 텍스트 분석, 단어 빈도 분석 등에 자주 사용됩니다.\n",
    "# 3. WordNet\n",
    "# 설명: WordNet은 단어의 의미 관계를 중심으로 구성된 어휘 데이터베이스로, 단어 간의 동의어 관계, 하위어-상위어 관계 등을 제공합니다. 각 단어는 의미에 따라 **신셋(synset)**이라는 그룹으로 묶여 있습니다.\n",
    "# 용도: 의미 분석, 자연어 이해 등의 연구에 널리 사용됩니다.\n",
    "# 4. Reuters Corpus\n",
    "# 설명: Reuters 코퍼스는 로이터 통신에서 발행한 뉴스 기사를 모은 데이터셋입니다. 경제, 정치, 스포츠 등 다양한 주제를 다루며, 약 10,000개의 뉴스 기사가 포함되어 있습니다.\n",
    "# 용도: 텍스트 분류, 정보 검색, 토픽 모델링 등의 연구에 사용됩니다.\n",
    "# 5. Movie Reviews Corpus\n",
    "# 설명: 영화 리뷰 데이터를 포함한 코퍼스로, 각 리뷰에는 긍정적 또는 부정적 감정 레이블이 달려 있습니다. 감정 분석 연구에 매우 유용합니다.\n",
    "# 용도: 감정 분석, 텍스트 분류 연구에 주로 사용됩니다.\n",
    "# 6. Inaugural Address Corpus\n",
    "# 설명: 미국 대통령들의 취임 연설 모음집으로, 1789년 조지 워싱턴부터 현대에 이르기까지 다양한 시대의 연설문이 포함되어 있습니다.\n",
    "# 용도: 언어 변화 분석, 정치적 담론 연구, 텍스트 분석 등에서 활용됩니다.\n",
    "# 7. Names Corpus\n",
    "# 설명: 사람의 이름과 성별이 매핑된 데이터셋입니다. 약 8,000개의 이름이 남성과 여성으로 분류되어 있습니다.\n",
    "# 용도: 성별 예측, 텍스트 분류 및 의미론적 연구에 사용됩니다.\n",
    "# 8. Conll2000 Chunking Corpus\n",
    "# 설명: 이 코퍼스는 텍스트에서 **문법적 단위(chunk)**를 예측하는 문제에 사용됩니다. 명사구, 동사구 등 문장의 다양한 문법적 요소를 분석하는 데 유용합니다.\n",
    "# 용도: 구문 분석, POS 태깅, 문법적 구조 학습에 사용됩니다.\n",
    "# 9. Twitter Samples\n",
    "# 설명: NLTK는 트위터 데이터를 제공하는데, 이는 소셜 미디어에서 사용되는 짧은 문장의 특성을 분석하는 데 유용합니다. 다양한 트윗 데이터를 통해 감정 분석, 키워드 분석 등을 수행할 수 있습니다.\n",
    "# 용도: 감정 분석, 트렌드 분석, 소셜 미디어 연구에 사용됩니다.\n",
    "# 10. Stopwords\n",
    "# 설명: NLTK는 영어를 포함한 여러 언어에서 자주 사용되지만 의미가 없는 불용어(stopwords) 리스트를 제공합니다.\n",
    "# 용도: 텍스트 전처리, 단어 빈도 분석에서 중요하지 않은 단어들을 제거하는 데 사용됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3887b4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10866,
     "status": "ok",
     "timestamp": 1729735713140,
     "user": {
      "displayName": "전윤범",
      "userId": "16579588953913211126"
     },
     "user_tz": -540
    },
    "id": "d3887b4b",
    "outputId": "2e9580af-a271-42c4-c556-2924cc204b69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['직장인에게', '파이썬은', '필수', '교육입니다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 설치 확인을 위한 간단한 예시\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# NLTK 데이터 다운로드\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 예시 문장\n",
    "text = \"직장인에게 파이썬은 필수 교육입니다.\"\n",
    "\n",
    "# 토큰화\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9f765fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1729735713141,
     "user": {
      "displayName": "전윤범",
      "userId": "16579588953913211126"
     },
     "user_tz": -540
    },
    "id": "f9f765fe",
    "outputId": "0778bd4a-d2e0-44e6-c79c-8ef6b8fb2931"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['직장인에게', '파이썬은', '필수', '교육입니다', '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82f512",
   "metadata": {
    "id": "5e82f512"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2cb8c9",
   "metadata": {
    "id": "6a2cb8c9"
   },
   "outputs": [],
   "source": [
    "## 토큰화 (Tokenization)\n",
    "# 토큰화는 텍스트 데이터를 분석하기 전에 문장을 개별 단어나 문장 단위로 분리하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d55f34a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1729735725438,
     "user": {
      "displayName": "전윤범",
      "userId": "16579588953913211126"
     },
     "user_tz": -540
    },
    "id": "d55f34a1",
    "outputId": "f3ab91a3-b869-4e85-e930-9f1c733a4290"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화: ['Hello', '!', 'How', 'are', 'you', '?', 'I', 'hope', 'you', \"'re\", 'doing', 'well', '.']\n",
      "문장 토큰화: ['Hello!', 'How are you?', \"I hope you're doing well.\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# 예시 텍스트\n",
    "text = \"Hello! How are you? I hope you're doing well.\"\n",
    "\n",
    "# 단어 토큰화\n",
    "word_tokens = word_tokenize(text)\n",
    "print(\"단어 토큰화:\", word_tokens)\n",
    "\n",
    "# 문장 토큰화\n",
    "sent_tokens = sent_tokenize(text)\n",
    "print(\"문장 토큰화:\", sent_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1495ae",
   "metadata": {
    "id": "8a1495ae"
   },
   "outputs": [],
   "source": [
    "# 품사 태깅 (POS Tagging)\n",
    "# 품사 태깅은 문장에서 각 단어의 품사를 식별하는 과정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a2d1f",
   "metadata": {
    "id": "4d4a2d1f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8542bf33",
   "metadata": {
    "id": "8542bf33"
   },
   "outputs": [],
   "source": [
    "# 3. 개체명 인식 (Named Entity Recognition, NER)\n",
    "# 개체명 인식은 텍스트에서 인물, 장소, 조직과 같은 고유명사를 식별하는 작업입니다. NLTK는 개체명 인식을 위한 사전 학습된 모델을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6f2459",
   "metadata": {
    "id": "ca6f2459"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8254ba",
   "metadata": {
    "id": "4a8254ba"
   },
   "outputs": [],
   "source": [
    "# 4. 어간 추출과 표제어 추출 (Stemming and Lemmatization)\n",
    "# 어간 추출(Stemming)은 단어의 어간(기본 형태)을 찾아내는 작업입니다. 어간 추출은 단어의 변형된 형태를 같은 기본 형태로 변환합니다.\n",
    "# 표제어 추출(Lemmatization)은 단어의 원형을 찾는 과정으로, 품사 정보를 활용하여 더 정확하게 처리됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c45da8",
   "metadata": {
    "id": "a9c45da8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b115fe2",
   "metadata": {
    "id": "8b115fe2"
   },
   "outputs": [],
   "source": [
    "# 5. 문서 분류 (Document Classification)\n",
    "# NLTK는 텍스트 데이터를 분류하는 데 사용할 수 있는 도구와 데이터를 제공합니다. 분류 작업은 문서가 긍정적인지, 부정적인지 또는 특정 카테고리에 속하는지를 예측하는 데 사용됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e285b",
   "metadata": {
    "id": "892e285b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1338c0",
   "metadata": {
    "id": "fd1338c0"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 6. 정서 분석 (Sentiment Analysis)\n",
    "# NLTK는 텍스트의 정서(긍정, 부정 등)을 분석할 수 있는 도구도 제공합니다. 정서 분석은 리뷰, 소셜 미디어 게시물, 고객 피드백 등의 텍스트 데이터에서 많이 사용됩니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e8883a",
   "metadata": {
    "id": "84e8883a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c496eca7",
   "metadata": {
    "id": "c496eca7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8cece4",
   "metadata": {
    "id": "df8cece4"
   },
   "outputs": [],
   "source": [
    "# 파이썬을 활용한 소문자 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "780e3b70",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1729735735742,
     "user": {
      "displayName": "전윤범",
      "userId": "16579588953913211126"
     },
     "user_tz": -540
    },
    "id": "780e3b70",
    "outputId": "66c1b3f2-13b4-4e6d-932a-f1323448270c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural language processing is interesting.\n"
     ]
    }
   ],
   "source": [
    "# 예시 문장\n",
    "text = \"Natural Language Processing is Interesting.\"\n",
    "\n",
    "# 소문자 변환\n",
    "text_lower = text.lower()\n",
    "\n",
    "# 결과 출력\n",
    "print(text_lower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca52a7a",
   "metadata": {
    "id": "aca52a7a"
   },
   "outputs": [],
   "source": [
    "# 파이썬을 활용한 구두점 제거 코드\n",
    "\n",
    "# 위 코드에서는 정규 표현식을 사용하여 알파벳, 숫자, 공백 이외의 모든 문자를 제거합니다.\n",
    "# 이 작업을 통해 텍스트에서 불필요한 구두점과 특수 기호가 제거되어 분석의 일관성이 향상됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98bea85a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1729735742854,
     "user": {
      "displayName": "전윤범",
      "userId": "16579588953913211126"
     },
     "user_tz": -540
    },
    "id": "98bea85a",
    "outputId": "c1cca02c-7397-4e47-ccd1-32010f1713f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world Hows it going\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 예시 문장\n",
    "text = \"Hello, world! How's it going?\"\n",
    "\n",
    "# 정규 표현식을 사용한 구두점 제거\n",
    "text_clean = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# 결과 출력\n",
    "print(text_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f96c96",
   "metadata": {
    "id": "90f96c96"
   },
   "outputs": [],
   "source": [
    "# 불용어(Stopwords) 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa4c77",
   "metadata": {
    "id": "67fa4c77"
   },
   "outputs": [],
   "source": [
    "# 불용어(Stopwords)는 문장에서 자주 등장하지만 의미 전달에 큰 기여를 하지 않는 단어들입니다.\n",
    "# 예를 들어, \"the\", \"is\", \"and\" 같은 단어는 문법적으로 중요하지만, 텍스트 분석에서는 큰 의미가 없을 수 있습니다. 이러한 불용어는 분석의 효율성을 높이기 위해 제거하는 것이 일반적입니다.\n",
    "# 이를 통해 텍스트에서 핵심 정보를 보다 쉽게 추출할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f6711fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1729735748661,
     "user": {
      "displayName": "전윤범",
      "userId": "16579588953913211126"
     },
     "user_tz": -540
    },
    "id": "0f6711fa",
    "outputId": "45e6e9a9-30b0-4017-bba6-55f634f67874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simple', 'sentence', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# NLTK 데이터 다운로드\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 예시 문장\n",
    "text = \"This is a simple sentence.\"\n",
    "\n",
    "# 단어 토큰화\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# 불용어 제거\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "# 결과 출력\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28d37c5",
   "metadata": {
    "id": "d28d37c5"
   },
   "outputs": [],
   "source": [
    "# (결과)\n",
    "# 영어 불용어 목록에서 the, is 같은 단어를 제거한 결과입니다.\n",
    "# 이를 통해 의미 없는 단어들이 제거되고, 텍스트의 핵심 단어만 남게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e3d2a78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 350,
     "status": "ok",
     "timestamp": 1729735755008,
     "user": {
      "displayName": "전윤범",
      "userId": "16579588953913211126"
     },
     "user_tz": -540
    },
    "id": "9e3d2a78",
    "outputId": "4605249f-26dd-492c-b42b-0f273749f15f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 문장: This is a simple example sentence for stopword removal.\n",
      "불용어 제거 후: simple example sentence stopword removal .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JYB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\JYB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# NLTK에서 제공하는 불용어 목록 로드\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 예시 문장\n",
    "sentence = \"This is a simple example sentence for stopword removal.\"\n",
    "\n",
    "# 토큰화\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# 불용어 제거\n",
    "filtered_sentence = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "# 결과 출력\n",
    "print(\"원본 문장:\", sentence)\n",
    "print(\"불용어 제거 후:\", \" \".join(filtered_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcdc61d",
   "metadata": {
    "id": "8fcdc61d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0746dac",
   "metadata": {
    "id": "f0746dac"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f0650c",
   "metadata": {
    "id": "61f0650c"
   },
   "outputs": [],
   "source": [
    "# 어간 추출(Stemming)은 단어의 접미사나 어형 변화를 제거하여 단어의 기본 형태(어간)를 추출하는 과정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93108d1",
   "metadata": {
    "id": "b93108d1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75399a1",
   "metadata": {
    "id": "e75399a1"
   },
   "outputs": [],
   "source": [
    "# 토큰화란?\n",
    "# 토큰화(Tokenization)는 자연어 처리(NLP)에서 텍스트를 의미 있는 단위로 나누는 과정을 말합니다. 이 단위를 토큰(Token)이라고 하며, 토큰은 주로 단어 또는 문장 단위로 나뉩니다.\n",
    "# 토큰화는 자연어 처리의 첫 번째 단계로, 텍스트 데이터를 분석하거나 처리하기 위해 필수적인 전처리 과정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af75395c",
   "metadata": {
    "executionInfo": {
     "elapsed": 396,
     "status": "ok",
     "timestamp": 1729735760943,
     "user": {
      "displayName": "전윤범",
      "userId": "16579588953913211126"
     },
     "user_tz": -540
    },
    "id": "af75395c"
   },
   "outputs": [],
   "source": [
    "## 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf183c3",
   "metadata": {
    "id": "bbf183c3",
    "outputId": "b07f2046-1e68-4902-c0c0-2c275b225c54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['고양이가', '쥐를', '잡았다', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 예시 문장\n",
    "sentence = \"고양이가 쥐를 잡았다.\"\n",
    "\n",
    "# 단어 토큰화\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908927f2",
   "metadata": {
    "id": "908927f2"
   },
   "outputs": [],
   "source": [
    "# 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fa0fd2",
   "metadata": {
    "id": "99fa0fd2"
   },
   "outputs": [],
   "source": [
    "# 2. 문장 토큰화\n",
    "# 문장 토큰화(Sentence Tokenization)는 텍스트를 문장 단위로 분리하는 방법입니다.\n",
    "# 보통 구두점(예: 마침표, 느낌표, 물음표)을 기준으로 문장을 나눕니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c6ff690",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1729735765413,
     "user": {
      "displayName": "전윤범",
      "userId": "16579588953913211126"
     },
     "user_tz": -540
    },
    "id": "3c6ff690",
    "outputId": "5ee831cb-ead1-44a7-f0a6-7909d353be98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘 날씨가 좋다.', '우리는 공원에 갈 것이다.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# 예시 문장\n",
    "text = \"오늘 날씨가 좋다. 우리는 공원에 갈 것이다.\"\n",
    "\n",
    "# 문장 토큰화\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2ea55f",
   "metadata": {
    "id": "6d2ea55f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c5e3360",
   "metadata": {
    "id": "0c5e3360"
   },
   "source": [
    "# 텍스트 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6f5a2",
   "metadata": {
    "id": "3db6f5a2"
   },
   "outputs": [],
   "source": [
    "# 텍스트 벡터화는 컴퓨터가 텍스트 데이터를 이해하고 처리할 수 있도록 텍스트를 숫자 형식(벡터)으로 변환하는 과정입니다.\n",
    "# 자연어는 사람이 이해하기에는 적합하지만, 기계가 처리하기 위해서는 수치 데이터로 변환해야 합니다.\n",
    "# 텍스트 벡터화는 머신러닝 모델이 텍스트 데이터를 입력으로 받아들일 수 있도록 하는 중요한 전처리 과정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b06c49",
   "metadata": {
    "id": "36b06c49"
   },
   "source": [
    "## Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f2a803",
   "metadata": {
    "id": "a1f2a803"
   },
   "outputs": [],
   "source": [
    "# Bag of Words(BoW)는 텍스트 데이터를 처리하는 대표적인 방법 중 하나로, 문서에서 등장하는 단어들의 빈도를 기반으로 벡터화하는 방식입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8078e4",
   "metadata": {
    "id": "0c8078e4"
   },
   "source": [
    "### NLTK를 사용한 BoW 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72803a3f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 426,
     "status": "ok",
     "timestamp": 1729735769986,
     "user": {
      "displayName": "전윤범",
      "userId": "16579588953913211126"
     },
     "user_tz": -540
    },
    "id": "72803a3f",
    "outputId": "d605fb89-8850-4d26-af50-4c2283db0c07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'i': 2, 'love': 2, 'nlp': 2, 'is': 1, 'fun': 1, 'learning': 1})\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# 예제 문장\n",
    "sentences = [\"I love NLP\", \"NLP is fun\", \"I love learning\"]\n",
    "\n",
    "# 모든 단어를 저장할 리스트\n",
    "all_words = []\n",
    "\n",
    "# 각 문장을 토큰화하여 단어 리스트를 생성\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence.lower())  # 소문자로 변환\n",
    "    all_words.extend(words)\n",
    "\n",
    "# 단어 빈도 계산\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# BoW 출력\n",
    "print(word_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad36613b",
   "metadata": {
    "id": "ad36613b"
   },
   "outputs": [],
   "source": [
    "# 이 코드는 세 문장에서 등장한 단어들의 빈도를 계산한 간단한 BoW 예제입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e849e09",
   "metadata": {
    "id": "2e849e09"
   },
   "source": [
    "### Scikit-learn을 사용한 BoW 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed88a990",
   "metadata": {
    "id": "ed88a990"
   },
   "outputs": [],
   "source": [
    "# Scikit-learn의 CountVectorizer를 사용하여 텍스트를 BoW로 변환하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1830c2",
   "metadata": {
    "id": "be1830c2"
   },
   "outputs": [],
   "source": [
    "# 각 문장은 BoW로 변환되어 행렬 형태로 표현됩니다.\n",
    "# get_feature_names_out()는 단어 사전(모든 고유한 단어)을 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb3dcc92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 371,
     "status": "ok",
     "timestamp": 1729735782383,
     "user": {
      "displayName": "전윤범",
      "userId": "16579588953913211126"
     },
     "user_tz": -540
    },
    "id": "cb3dcc92",
    "outputId": "6755235b-1df6-4ca8-964e-af9968b56424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1]\n",
      " [1 1 0 0 1]\n",
      " [0 0 1 1 0]]\n",
      "['fun' 'is' 'learning' 'love' 'nlp']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 예제 문장\n",
    "sentences = [\"I love NLP\", \"NLP is fun\", \"I love learning\"]\n",
    "\n",
    "# CountVectorizer 객체 생성\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# 문장을 BoW로 변환\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# BoW 출력 (각 문장에 대한 단어 빈도 행렬)\n",
    "print(X.toarray())\n",
    "\n",
    "# 단어 사전 출력\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbd5218",
   "metadata": {
    "id": "ebbd5218"
   },
   "outputs": [],
   "source": [
    "# [[1 0 1 0 1 0]  # 첫 번째 문장: \"I love NLP\"\n",
    "#  [0 1 0 1 0 1]  # 두 번째 문장: \"NLP is fun\"\n",
    "#  [1 0 1 1 0 0]] # 세 번째 문장: \"I love learning\"\n",
    "# 이 행렬에서 각 행은 문장 하나를 나타내고, 열은 단어 사전에서 각 단어의 출현 빈도를 나타냅니다.\n",
    "# get_feature_names_out()를 통해 확인한 단어 사전은 ['fun', 'is', 'learning', 'love', 'nlp', 'i']로 구성됩니다.\n",
    "# 이 사전의 각 단어가 열에 해당합니다.\n",
    "\n",
    "# 첫 번째 문장 **\"I love NLP\"**는 i, love, nlp 단어들이 한 번씩 등장하므로, 이 단어들의 위치에 1이 나타납니다.\n",
    "# 두 번째 문장 **\"NLP is fun\"**는 is, fun, nlp 단어들이 등장하므로, 해당 단어들의 위치에 1이 표시됩니다.\n",
    "# 세 번째 문장 **\"I love learning\"**는 i, love, learning 단어들이 등장하므로, 그 위치에 1이 표시됩니다.\n",
    "# 이처럼 Scikit-learn의 BoW 구현에서는 각 문장이 단어 사전에 있는 단어들 중에서 몇 번 등장했는지를 벡터로 나타내어, 각 문장에 대한 특징 벡터를 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0ee703",
   "metadata": {
    "id": "3e0ee703"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98d9534",
   "metadata": {
    "id": "b98d9534"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af9f5cb4",
   "metadata": {
    "id": "af9f5cb4"
   },
   "source": [
    "## TF-IDF(Term Frequency-Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd217699",
   "metadata": {
    "id": "dd217699"
   },
   "outputs": [],
   "source": [
    "# TF-IDF(Term Frequency-Inverse Document Frequency)는\n",
    "# 문서 내 단어의 빈도와, 그 단어가 전체 문서 집합에서 얼마나 자주 등장하는지를 동시에 고려하여 단어의 중요도를 계산하는 방법입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8836b693",
   "metadata": {
    "id": "8836b693"
   },
   "outputs": [],
   "source": [
    "# Scikit-learn의 TfidfVectorizer를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "669b190b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1729735786836,
     "user": {
      "displayName": "전윤범",
      "userId": "16579588953913211126"
     },
     "user_tz": -540
    },
    "id": "669b190b",
    "outputId": "35b0e01e-2f31-46b0-943a-126cdf58f4f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.70710678 0.70710678]\n",
      " [0.62276601 0.62276601 0.         0.         0.4736296 ]\n",
      " [0.         0.         0.79596054 0.60534851 0.        ]]\n",
      "['fun' 'is' 'learning' 'love' 'nlp']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 예제 문장\n",
    "sentences = [\"I love NLP\", \"NLP is fun\", \"I love learning\"]\n",
    "\n",
    "# TfidfVectorizer 객체 생성\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 문장을 TF-IDF로 변환\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# TF-IDF 행렬 출력\n",
    "print(X.toarray())\n",
    "\n",
    "# 단어 사전 출력\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f4ec76",
   "metadata": {
    "id": "90f4ec76"
   },
   "outputs": [],
   "source": [
    "# 결과:\n",
    "# TF-IDF 행렬 출력 (각 문장에 대한 단어 빈도 및 가중치):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7703f738",
   "metadata": {
    "id": "7703f738"
   },
   "outputs": [],
   "source": [
    "# 설명:\n",
    "# 단어 사전: ['fun', 'is', 'learning', 'love', 'nlp']는 코퍼스에 포함된 모든 고유한 단어들입니다.\n",
    "# TF-IDF 행렬: 각 행은 문장 하나를 나타내고, 각 열은 단어 사전에서 해당 단어의 TF-IDF 값을 나타냅니다. 이 값은 단어가 문서에서 얼마나 중요한지를 나타내며, 단순 빈도보다 문서 내 의미 있는 단어를 더 잘 나타냅니다.\n",
    "# 예를 들어, 첫 번째 문장 **\"I love NLP\"**는 love와 nlp 단어들의 중요도가 높아 그 단어들의 위치에 높은 값이 들어갑니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5bc51e",
   "metadata": {
    "id": "9b5bc51e"
   },
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c58aa7a",
   "metadata": {
    "id": "0c58aa7a"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Word Embedding은 단어를 고차원의 벡터 공간에 매핑하는 방식으로, 단어 간의 의미적 관계를 반영합니다. 대표적인 방법으로 Word2Vec, GloVe, FastText 등이 있습니다.\n",
    "# 이 방법들은 단어 간의 의미적 유사성을 반영하여, 자연어 처리에서 더욱 효율적으로 텍스트 데이터를 처리할 수 있게 해줍니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d4f3fc",
   "metadata": {
    "id": "02d4f3fc"
   },
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30bee70",
   "metadata": {
    "id": "d30bee70"
   },
   "outputs": [],
   "source": [
    "# Word2Vec은 단어의 의미적 유사성을 반영하기 위해, 단어를 밀집 벡터로 학습하는 방법입니다.\n",
    "# 이 모델은 CBOW (Continuous Bag of Words)와 Skip-gram이라는 두 가지 학습 방식을 통해 단어 간의 관계를 학습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a46e82",
   "metadata": {
    "id": "84a46e82"
   },
   "source": [
    "CBOW (Continuous Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a87e7",
   "metadata": {
    "id": "f08a87e7"
   },
   "outputs": [],
   "source": [
    "#  **문맥(context)**을 사용하여 **중심 단어(center word)**를 예측하는 방식입니다.\n",
    "#     이 모델은 중심 단어 주위의 여러 단어들(문맥)을 기반으로 단어의 의미를 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad519864",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3866,
     "status": "ok",
     "timestamp": 1729735797591,
     "user": {
      "displayName": "전윤범",
      "userId": "16579588953913211126"
     },
     "user_tz": -540
    },
    "id": "ad519864",
    "outputId": "48e82f8f-d831-4351-cd85-22b4ada750de",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# 필요 라이브러리 설치\n",
    "!pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59588723",
   "metadata": {
    "id": "59588723"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21b6c8a",
   "metadata": {
    "id": "c21b6c8a"
   },
   "outputs": [],
   "source": [
    "# 코드 설명:\n",
    "# 입력 데이터: 입력으로 세 개의 간단한 문장이 사용됩니다. 이 문장들은 먼저 word_tokenize로 토큰화됩니다.\n",
    "\n",
    "# CBOW 모델: Word2Vec의 sg=0 파라미터는 CBOW 방식을 지정합니다. (반면, sg=1이면 Skip-gram 방식입니다). vector_size=50은 각 단어 벡터의 차원을 50으로 설정합니다. window=2는 중심 단어를 예측할 때 양쪽으로 두 개의 단어씩 문맥으로 본다는 뜻입니다.\n",
    "\n",
    "# 단어 벡터 및 유사도 확인: 학습이 끝나면 'nlp'라는 단어의 벡터를 출력하고, 'nlp'와 유사한 단어들을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e67068b4",
   "metadata": {
    "executionInfo": {
     "elapsed": 327,
     "status": "ok",
     "timestamp": 1729735832951,
     "user": {
      "displayName": "전윤범",
      "userId": "16579588953913211126"
     },
     "user_tz": -540
    },
    "id": "e67068b4"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "335529ba",
   "metadata": {
    "id": "335529ba",
    "outputId": "27bc6735-f889-4e38-ca0d-d379fff8a573"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP 단어의 벡터:\n",
      " [-1.0724545e-03  4.7286032e-04  1.0206699e-02  1.8018546e-02\n",
      " -1.8605899e-02 -1.4233618e-02  1.2917743e-02  1.7945977e-02\n",
      " -1.0030856e-02 -7.5267460e-03  1.4761009e-02 -3.0669451e-03\n",
      " -9.0732286e-03  1.3108101e-02 -9.7203208e-03 -3.6320353e-03\n",
      "  5.7531595e-03  1.9837476e-03 -1.6570430e-02 -1.8897638e-02\n",
      "  1.4623532e-02  1.0140524e-02  1.3515387e-02  1.5257311e-03\n",
      "  1.2701779e-02 -6.8107317e-03 -1.8928051e-03  1.1537147e-02\n",
      " -1.5043277e-02 -7.8722099e-03 -1.5023164e-02 -1.8600845e-03\n",
      "  1.9076237e-02 -1.4638334e-02 -4.6675396e-03 -3.8754845e-03\n",
      "  1.6154870e-02 -1.1861792e-02  9.0322494e-05 -9.5074698e-03\n",
      " -1.9207101e-02  1.0014586e-02 -1.7519174e-02 -8.7836506e-03\n",
      " -7.0199967e-05 -5.9236528e-04 -1.5322480e-02  1.9229483e-02\n",
      "  9.9641131e-03  1.8466286e-02]\n",
      "NLP와 유사한 단어들:\n",
      " [('i', 0.1267007291316986), ('love', 0.04237299785017967), ('is', 0.012442132458090782), ('learning', -0.014475276693701744), ('fun', -0.11821283400058746)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 예제 문장들\n",
    "sentences = [\"I love NLP\", \"NLP is fun\", \"I love learning\"]\n",
    "\n",
    "# 문장 토큰화\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# CBOW 모델 학습 (Word2Vec의 기본값이 CBOW)\n",
    "cbow_model = Word2Vec(sentences=tokenized_sentences, vector_size=50, window=2, min_count=1, sg=0)\n",
    "\n",
    "# 'nlp' 단어의 벡터 확인\n",
    "print(\"NLP 단어의 벡터:\\n\", cbow_model.wv['nlp'])\n",
    "\n",
    "# 가장 유사한 단어 확인\n",
    "print(\"NLP와 유사한 단어들:\\n\", cbow_model.wv.most_similar('nlp'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8f07bf",
   "metadata": {
    "id": "7d8f07bf"
   },
   "outputs": [],
   "source": [
    "# 결과:\n",
    "# 이 코드를 실행하면 NLP 단어의 벡터 값과 NLP와 유사한 단어들이 출력됩니다.\n",
    "# 이는 모델이 문맥을 학습하여 단어 간의 의미적 유사성을 어떻게 반영하는지 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7100a1fa",
   "metadata": {
    "id": "7100a1fa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0df90f82",
   "metadata": {
    "id": "0df90f82"
   },
   "source": [
    "Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93838bd5",
   "metadata": {
    "id": "93838bd5"
   },
   "outputs": [],
   "source": [
    "# 중심 단어를 이용해 주변 단어를 예측하는 방식입니다.\n",
    "# 이 방식을 통해 단어가 문장에서 사용되는 맥락에 대해 더 깊이 학습할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c9e77f",
   "metadata": {
    "id": "34c9e77f"
   },
   "outputs": [],
   "source": [
    "# 코드 설명:\n",
    "# 입력 데이터: 입력 문장들을 토큰화한 후 모델에 학습 데이터를 제공합니다.\n",
    "\n",
    "# Skip-gram 모델 설정: sg=1로 설정하면 Skip-gram 방식으로 학습됩니다. 나머지 파라미터는 CBOW와 동일하게 설정되었지만, Skip-gram은 중심 단어로 주변 단어들을 예측하기 때문에 작은 데이터셋에서 더 효과적일 수 있습니다.\n",
    "\n",
    "# vector_size=50: 단어 벡터의 차원을 50으로 설정.\n",
    "# window=2: 중심 단어를 기준으로 양쪽 2개의 단어까지 문맥으로 사용.\n",
    "# min_count=1: 1번 이상 등장한 단어만 학습에 포함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a7c736c",
   "metadata": {
    "id": "9a7c736c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP 단어의 벡터:\n",
      " [-1.0724545e-03  4.7286032e-04  1.0206699e-02  1.8018546e-02\n",
      " -1.8605899e-02 -1.4233618e-02  1.2917743e-02  1.7945977e-02\n",
      " -1.0030856e-02 -7.5267460e-03  1.4761009e-02 -3.0669451e-03\n",
      " -9.0732286e-03  1.3108101e-02 -9.7203208e-03 -3.6320353e-03\n",
      "  5.7531595e-03  1.9837476e-03 -1.6570430e-02 -1.8897638e-02\n",
      "  1.4623532e-02  1.0140524e-02  1.3515387e-02  1.5257311e-03\n",
      "  1.2701779e-02 -6.8107317e-03 -1.8928051e-03  1.1537147e-02\n",
      " -1.5043277e-02 -7.8722099e-03 -1.5023164e-02 -1.8600845e-03\n",
      "  1.9076237e-02 -1.4638334e-02 -4.6675396e-03 -3.8754845e-03\n",
      "  1.6154870e-02 -1.1861792e-02  9.0322494e-05 -9.5074698e-03\n",
      " -1.9207101e-02  1.0014586e-02 -1.7519174e-02 -8.7836506e-03\n",
      " -7.0199967e-05 -5.9236528e-04 -1.5322480e-02  1.9229483e-02\n",
      "  9.9641131e-03  1.8466286e-02]\n",
      "NLP와 유사한 단어들:\n",
      " [('i', 0.1267007291316986), ('love', 0.04237299785017967), ('is', 0.012442132458090782), ('learning', -0.014475276693701744), ('fun', -0.11821283400058746)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 예제 문장들\n",
    "sentences = [\"I love NLP\", \"NLP is fun\", \"I love learning\"]\n",
    "\n",
    "# 문장 토큰화\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Skip-gram 모델 학습 (sg=1 설정으로 Skip-gram 방식 사용)\n",
    "skipgram_model = Word2Vec(sentences=tokenized_sentences, vector_size=50, window=2, min_count=1, sg=1)\n",
    "\n",
    "# 'nlp' 단어의 벡터 확인\n",
    "print(\"NLP 단어의 벡터:\\n\", skipgram_model.wv['nlp'])\n",
    "\n",
    "# 가장 유사한 단어 확인\n",
    "print(\"NLP와 유사한 단어들:\\n\", skipgram_model.wv.most_similar('nlp'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8122e9",
   "metadata": {
    "id": "7d8122e9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f25af8",
   "metadata": {
    "id": "53f25af8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee8346",
   "metadata": {
    "id": "b2ee8346"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
